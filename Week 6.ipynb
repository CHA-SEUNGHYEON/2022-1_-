{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cdce902",
   "metadata": {},
   "source": [
    "# Logistic (regression) classifier: Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17e72707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8a3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d236338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('diabetes.csv', delimiter=',', dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44cfba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "#Placeholders for a tensor that will be always fed.\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246fe0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "#Accuracy computation\n",
    "#True if hypothsis > 0.5, else fALSE\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c5ddc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nan\n",
      "200 nan\n",
      "400 nan\n",
      "600 nan\n",
      "800 nan\n",
      "1000 nan\n",
      "1200 nan\n",
      "1400 nan\n",
      "1600 nan\n",
      "1800 nan\n",
      "2000 nan\n",
      "2200 nan\n",
      "2400 nan\n",
      "2600 nan\n",
      "2800 nan\n",
      "3000 nan\n",
      "3200 nan\n",
      "3400 nan\n",
      "3600 nan\n",
      "3800 nan\n",
      "4000 nan\n",
      "4200 nan\n",
      "4400 nan\n",
      "4600 nan\n",
      "4800 nan\n",
      "5000 nan\n",
      "5200 nan\n",
      "5400 nan\n",
      "5600 nan\n",
      "5800 nan\n",
      "6000 nan\n",
      "6200 nan\n",
      "6400 nan\n",
      "6600 nan\n",
      "6800 nan\n",
      "7000 nan\n",
      "7200 nan\n",
      "7400 nan\n",
      "7600 nan\n",
      "7800 nan\n",
      "8000 nan\n",
      "8200 nan\n",
      "8400 nan\n",
      "8600 nan\n",
      "8800 nan\n",
      "9000 nan\n",
      "9200 nan\n",
      "9400 nan\n",
      "9600 nan\n",
      "9800 nan\n",
      "10000 nan\n",
      "\n",
      " Hypothesis:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]] \n",
      " Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      " Accuracy:  0.6510417\n"
     ]
    }
   ],
   "source": [
    "#Launch Graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    feed = {X: x_data, Y: y_data}\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict=feed)\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict=feed))\n",
    "            \n",
    "    #Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict=feed)\n",
    "    print('\\n Hypothesis: ', h, '\\n Correct (Y): ', c, '\\n Accuracy: ', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a17cc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.mean(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "x_data = MinMaxScaler(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c57bb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placeholders for a tensor that will be always fed.\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1130c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "#Accuracy computation\n",
    "#True if hypothsis > 0.5, else fALSE\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "687b9f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.67066884\n",
      "200 0.66562724\n",
      "400 0.66083986\n",
      "600 0.65624374\n",
      "800 0.6518092\n",
      "1000 0.6475206\n",
      "1200 0.6433688\n",
      "1400 0.63934743\n",
      "1600 0.6354514\n",
      "1800 0.63167614\n",
      "2000 0.62801754\n",
      "2200 0.6244716\n",
      "2400 0.6210346\n",
      "2600 0.6177028\n",
      "2800 0.6144726\n",
      "3000 0.61134076\n",
      "3200 0.60830384\n",
      "3400 0.60535854\n",
      "3600 0.60250187\n",
      "3800 0.5997309\n",
      "4000 0.59704244\n",
      "4200 0.59443384\n",
      "4400 0.5919025\n",
      "4600 0.58944553\n",
      "4800 0.5870605\n",
      "5000 0.584745\n",
      "5200 0.58249646\n",
      "5400 0.5803128\n",
      "5600 0.5781916\n",
      "5800 0.57613087\n",
      "6000 0.57412845\n",
      "6200 0.57218236\n",
      "6400 0.5702906\n",
      "6600 0.56845146\n",
      "6800 0.5666631\n",
      "7000 0.5649237\n",
      "7200 0.56323165\n",
      "7400 0.56158537\n",
      "7600 0.5599832\n",
      "7800 0.5584238\n",
      "8000 0.5569057\n",
      "8200 0.5554274\n",
      "8400 0.55398756\n",
      "8600 0.55258507\n",
      "8800 0.55121857\n",
      "9000 0.5498869\n",
      "9200 0.5485888\n",
      "9400 0.5473234\n",
      "9600 0.54608935\n",
      "9800 0.5448858\n",
      "10000 0.5437118\n",
      "10200 0.54256624\n",
      "10400 0.5414483\n",
      "10600 0.5403571\n",
      "10800 0.53929174\n",
      "11000 0.53825146\n",
      "11200 0.53723544\n",
      "11400 0.5362429\n",
      "11600 0.5352731\n",
      "11800 0.53432536\n",
      "12000 0.533399\n",
      "12200 0.5324934\n",
      "12400 0.53160787\n",
      "12600 0.5307419\n",
      "12800 0.5298947\n",
      "13000 0.5290659\n",
      "13200 0.5282549\n",
      "13400 0.5274611\n",
      "13600 0.5266841\n",
      "13800 0.52592343\n",
      "14000 0.5251785\n",
      "14200 0.52444893\n",
      "14400 0.5237343\n",
      "14600 0.52303404\n",
      "14800 0.5223479\n",
      "15000 0.5216755\n",
      "15200 0.52101636\n",
      "15400 0.5203702\n",
      "15600 0.5197366\n",
      "15800 0.51911515\n",
      "16000 0.51850563\n",
      "16200 0.5179078\n",
      "16400 0.5173211\n",
      "16600 0.51674545\n",
      "16800 0.51618046\n",
      "17000 0.5156259\n",
      "17200 0.51508147\n",
      "17400 0.51454693\n",
      "17600 0.51402193\n",
      "17800 0.5135065\n",
      "18000 0.5130001\n",
      "18200 0.5125025\n",
      "18400 0.5120137\n",
      "18600 0.51153344\n",
      "18800 0.5110614\n",
      "19000 0.5105973\n",
      "19200 0.5101412\n",
      "19400 0.5096927\n",
      "19600 0.50925183\n",
      "19800 0.5088182\n",
      "20000 0.5083918\n",
      "\n",
      " Hypothesis:  [[0.5987338 ]\n",
      " [0.13117138]\n",
      " [0.64452153]\n",
      " [0.11725435]\n",
      " [0.57536346]\n",
      " [0.24566475]\n",
      " [0.13819379]\n",
      " [0.36453444]\n",
      " [0.73573154]\n",
      " [0.32098943]\n",
      " [0.25694644]\n",
      " [0.6915529 ]\n",
      " [0.70168626]\n",
      " [0.74429476]\n",
      " [0.6258472 ]\n",
      " [0.26387042]\n",
      " [0.334888  ]\n",
      " [0.27450955]\n",
      " [0.23374957]\n",
      " [0.26713434]\n",
      " [0.39825612]\n",
      " [0.38111138]\n",
      " [0.7659081 ]\n",
      " [0.37882787]\n",
      " [0.6870233 ]\n",
      " [0.49458113]\n",
      " [0.5517622 ]\n",
      " [0.13963854]\n",
      " [0.6771817 ]\n",
      " [0.33162254]\n",
      " [0.43917486]\n",
      " [0.5202432 ]\n",
      " [0.12797838]\n",
      " [0.16431001]\n",
      " [0.5040945 ]\n",
      " [0.2857577 ]\n",
      " [0.55220693]\n",
      " [0.441603  ]\n",
      " [0.19469222]\n",
      " [0.5455176 ]\n",
      " [0.530328  ]\n",
      " [0.51257634]\n",
      " [0.31724247]\n",
      " [0.8316779 ]\n",
      " [0.5312259 ]\n",
      " [0.6691974 ]\n",
      " [0.3256795 ]\n",
      " [0.10974869]\n",
      " [0.3293635 ]\n",
      " [0.14128229]\n",
      " [0.13956314]\n",
      " [0.15501687]\n",
      " [0.17690435]\n",
      " [0.79482603]\n",
      " [0.65477675]\n",
      " [0.07659283]\n",
      " [0.75870633]\n",
      " [0.3015398 ]\n",
      " [0.5583711 ]\n",
      " [0.1933425 ]\n",
      " [0.06147069]\n",
      " [0.45790544]\n",
      " [0.09939069]\n",
      " [0.33577937]\n",
      " [0.3604675 ]\n",
      " [0.22350478]\n",
      " [0.26017788]\n",
      " [0.39350364]\n",
      " [0.11893618]\n",
      " [0.37578708]\n",
      " [0.23273677]\n",
      " [0.39153144]\n",
      " [0.6469451 ]\n",
      " [0.33641633]\n",
      " [0.11677241]\n",
      " [0.02164844]\n",
      " [0.1827918 ]\n",
      " [0.24319339]\n",
      " [0.2624984 ]\n",
      " [0.18281028]\n",
      " [0.16910577]\n",
      " [0.04827845]\n",
      " [0.27063859]\n",
      " [0.12450725]\n",
      " [0.48315367]\n",
      " [0.2575145 ]\n",
      " [0.5293055 ]\n",
      " [0.20770863]\n",
      " [0.6935205 ]\n",
      " [0.1515406 ]\n",
      " [0.07829013]\n",
      " [0.34756833]\n",
      " [0.32461327]\n",
      " [0.41704646]\n",
      " [0.31610045]\n",
      " [0.5195333 ]\n",
      " [0.14318955]\n",
      " [0.07799664]\n",
      " [0.2070418 ]\n",
      " [0.37283587]\n",
      " [0.53994954]\n",
      " [0.26769087]\n",
      " [0.16518858]\n",
      " [0.1059646 ]\n",
      " [0.19369707]\n",
      " [0.26887536]\n",
      " [0.12322745]\n",
      " [0.4256685 ]\n",
      " [0.15712348]\n",
      " [0.15161827]\n",
      " [0.48342866]\n",
      " [0.6979961 ]\n",
      " [0.13089398]\n",
      " [0.14208722]\n",
      " [0.6223737 ]\n",
      " [0.5483625 ]\n",
      " [0.3461304 ]\n",
      " [0.1725601 ]\n",
      " [0.18292275]\n",
      " [0.15863022]\n",
      " [0.556476  ]\n",
      " [0.28728092]\n",
      " [0.21407908]\n",
      " [0.47988215]\n",
      " [0.17069665]\n",
      " [0.23377901]\n",
      " [0.35478026]\n",
      " [0.22177541]\n",
      " [0.2969347 ]\n",
      " [0.28523463]\n",
      " [0.5456039 ]\n",
      " [0.49389574]\n",
      " [0.54308254]\n",
      " [0.31269565]\n",
      " [0.15511724]\n",
      " [0.2747715 ]\n",
      " [0.15785527]\n",
      " [0.13610819]\n",
      " [0.26314968]\n",
      " [0.29204965]\n",
      " [0.3336519 ]\n",
      " [0.32575923]\n",
      " [0.19659197]\n",
      " [0.40063578]\n",
      " [0.4410048 ]\n",
      " [0.08765984]\n",
      " [0.19601378]\n",
      " [0.32492062]\n",
      " [0.570991  ]\n",
      " [0.11886647]\n",
      " [0.34439152]\n",
      " [0.24785852]\n",
      " [0.73371077]\n",
      " [0.49338692]\n",
      " [0.7681372 ]\n",
      " [0.64021254]\n",
      " [0.16213599]\n",
      " [0.20195252]\n",
      " [0.1295245 ]\n",
      " [0.87913346]\n",
      " [0.4502305 ]\n",
      " [0.38001752]\n",
      " [0.25921804]\n",
      " [0.16155764]\n",
      " [0.2879202 ]\n",
      " [0.35587385]\n",
      " [0.34951836]\n",
      " [0.31748763]\n",
      " [0.24989268]\n",
      " [0.24093667]\n",
      " [0.2557239 ]\n",
      " [0.44583982]\n",
      " [0.15449178]\n",
      " [0.17024931]\n",
      " [0.13491124]\n",
      " [0.73433846]\n",
      " [0.23503992]\n",
      " [0.4372447 ]\n",
      " [0.5275379 ]\n",
      " [0.47322562]\n",
      " [0.1640847 ]\n",
      " [0.24110267]\n",
      " [0.02494362]\n",
      " [0.12652442]\n",
      " [0.37633836]\n",
      " [0.7793318 ]\n",
      " [0.82662284]\n",
      " [0.39485776]\n",
      " [0.3704198 ]\n",
      " [0.40246117]\n",
      " [0.15849602]\n",
      " [0.49704194]\n",
      " [0.5389043 ]\n",
      " [0.66236174]\n",
      " [0.2316415 ]\n",
      " [0.5650121 ]\n",
      " [0.12814555]\n",
      " [0.19730517]\n",
      " [0.32318348]\n",
      " [0.42242888]\n",
      " [0.20480803]\n",
      " [0.31088883]\n",
      " [0.20421866]\n",
      " [0.14097393]\n",
      " [0.42551175]\n",
      " [0.2434619 ]\n",
      " [0.8599429 ]\n",
      " [0.5986558 ]\n",
      " [0.15208545]\n",
      " [0.7157576 ]\n",
      " [0.11824244]\n",
      " [0.37451112]\n",
      " [0.7449241 ]\n",
      " [0.3418296 ]\n",
      " [0.42484564]\n",
      " [0.76549083]\n",
      " [0.30863553]\n",
      " [0.3857745 ]\n",
      " [0.25414777]\n",
      " [0.32963997]\n",
      " [0.56672686]\n",
      " [0.60895103]\n",
      " [0.30935225]\n",
      " [0.64388746]\n",
      " [0.16177213]\n",
      " [0.12591922]\n",
      " [0.15036732]\n",
      " [0.49490324]\n",
      " [0.8776852 ]\n",
      " [0.2332708 ]\n",
      " [0.4297506 ]\n",
      " [0.59120476]\n",
      " [0.11136594]\n",
      " [0.3012691 ]\n",
      " [0.12022021]\n",
      " [0.55973756]\n",
      " [0.7726762 ]\n",
      " [0.5371987 ]\n",
      " [0.6557821 ]\n",
      " [0.1343661 ]\n",
      " [0.11989826]\n",
      " [0.19431055]\n",
      " [0.27733624]\n",
      " [0.43610018]\n",
      " [0.41753185]\n",
      " [0.8118661 ]\n",
      " [0.45231172]\n",
      " [0.57905126]\n",
      " [0.5074701 ]\n",
      " [0.16991457]\n",
      " [0.37205344]\n",
      " [0.24502191]\n",
      " [0.126598  ]\n",
      " [0.12805185]\n",
      " [0.4645724 ]\n",
      " [0.21363533]\n",
      " [0.25978935]\n",
      " [0.18934777]\n",
      " [0.56183344]\n",
      " [0.8007996 ]\n",
      " [0.6040769 ]\n",
      " [0.35106158]\n",
      " [0.21328601]\n",
      " [0.4942828 ]\n",
      " [0.29865062]\n",
      " [0.34381527]\n",
      " [0.31404728]\n",
      " [0.38418743]\n",
      " [0.10885164]\n",
      " [0.29351288]\n",
      " [0.5447134 ]\n",
      " [0.16183561]\n",
      " [0.26008558]\n",
      " [0.11226559]\n",
      " [0.5154202 ]\n",
      " [0.2526526 ]\n",
      " [0.26642853]\n",
      " [0.15853202]\n",
      " [0.39192486]\n",
      " [0.22283453]\n",
      " [0.31973237]\n",
      " [0.5413131 ]\n",
      " [0.4553716 ]\n",
      " [0.5781486 ]\n",
      " [0.2516883 ]\n",
      " [0.544514  ]\n",
      " [0.6367115 ]\n",
      " [0.37051314]\n",
      " [0.16226554]\n",
      " [0.31112272]\n",
      " [0.11848757]\n",
      " [0.22154278]\n",
      " [0.46749   ]\n",
      " [0.33828807]\n",
      " [0.45366955]\n",
      " [0.54026693]\n",
      " [0.38718855]\n",
      " [0.25587967]\n",
      " [0.5575992 ]\n",
      " [0.45986557]\n",
      " [0.43134236]\n",
      " [0.35563725]\n",
      " [0.19720063]\n",
      " [0.36137512]\n",
      " [0.3441583 ]\n",
      " [0.29451638]\n",
      " [0.6658092 ]\n",
      " [0.21413594]\n",
      " [0.3326409 ]\n",
      " [0.34946185]\n",
      " [0.20641595]\n",
      " [0.21892646]\n",
      " [0.37447727]\n",
      " [0.24350753]\n",
      " [0.47519246]\n",
      " [0.22943676]\n",
      " [0.16012397]\n",
      " [0.5165428 ]\n",
      " [0.2817454 ]\n",
      " [0.70809186]\n",
      " [0.34783566]\n",
      " [0.22096798]\n",
      " [0.22846341]\n",
      " [0.71700287]\n",
      " [0.20504299]\n",
      " [0.32410103]\n",
      " [0.30970043]\n",
      " [0.7017345 ]\n",
      " [0.2277391 ]\n",
      " [0.2899235 ]\n",
      " [0.5131968 ]\n",
      " [0.1374366 ]\n",
      " [0.56414914]\n",
      " [0.38889775]\n",
      " [0.12171569]\n",
      " [0.502343  ]\n",
      " [0.29412135]\n",
      " [0.33222157]\n",
      " [0.659883  ]\n",
      " [0.68915987]\n",
      " [0.2361606 ]\n",
      " [0.18717188]\n",
      " [0.02971068]\n",
      " [0.329197  ]\n",
      " [0.40953144]\n",
      " [0.55380404]\n",
      " [0.29390615]\n",
      " [0.1760605 ]\n",
      " [0.15941972]\n",
      " [0.06566867]\n",
      " [0.21341127]\n",
      " [0.33708802]\n",
      " [0.14912799]\n",
      " [0.13628271]\n",
      " [0.1923551 ]\n",
      " [0.6631451 ]\n",
      " [0.33544612]\n",
      " [0.6560552 ]\n",
      " [0.44734308]\n",
      " [0.675861  ]\n",
      " [0.68232024]\n",
      " [0.58888245]\n",
      " [0.43936962]\n",
      " [0.6092486 ]\n",
      " [0.46661842]\n",
      " [0.26619476]\n",
      " [0.3138475 ]\n",
      " [0.11165804]\n",
      " [0.12636614]\n",
      " [0.36288956]\n",
      " [0.7620789 ]\n",
      " [0.18207401]\n",
      " [0.13067484]\n",
      " [0.20841405]\n",
      " [0.3452891 ]\n",
      " [0.77617884]\n",
      " [0.12444597]\n",
      " [0.15922108]\n",
      " [0.5175365 ]\n",
      " [0.26527447]\n",
      " [0.2198847 ]\n",
      " [0.1200864 ]\n",
      " [0.20266798]\n",
      " [0.17545626]\n",
      " [0.21093383]\n",
      " [0.17156878]\n",
      " [0.35489795]\n",
      " [0.44025925]\n",
      " [0.597137  ]\n",
      " [0.25266033]\n",
      " [0.24026373]\n",
      " [0.56166095]\n",
      " [0.24982345]\n",
      " [0.27501887]\n",
      " [0.5013093 ]\n",
      " [0.39233506]\n",
      " [0.25652784]\n",
      " [0.23792824]\n",
      " [0.11359838]\n",
      " [0.58362883]\n",
      " [0.18610427]\n",
      " [0.44141063]\n",
      " [0.43985862]\n",
      " [0.23664242]\n",
      " [0.5425124 ]\n",
      " [0.33349806]\n",
      " [0.31130004]\n",
      " [0.11951876]\n",
      " [0.78136003]\n",
      " [0.6281806 ]\n",
      " [0.3160901 ]\n",
      " [0.23926958]\n",
      " [0.45791727]\n",
      " [0.26299244]\n",
      " [0.30312482]\n",
      " [0.5481596 ]\n",
      " [0.17951366]\n",
      " [0.49186978]\n",
      " [0.10430291]\n",
      " [0.28054637]\n",
      " [0.32277274]\n",
      " [0.15090987]\n",
      " [0.19795167]\n",
      " [0.21018049]\n",
      " [0.6551749 ]\n",
      " [0.64629054]\n",
      " [0.06437466]\n",
      " [0.5671718 ]\n",
      " [0.32518652]\n",
      " [0.22057536]\n",
      " [0.11744192]\n",
      " [0.21105084]\n",
      " [0.11729115]\n",
      " [0.26675415]\n",
      " [0.18691209]\n",
      " [0.30215222]\n",
      " [0.63935107]\n",
      " [0.40438086]\n",
      " [0.10327238]\n",
      " [0.33266562]\n",
      " [0.5868139 ]\n",
      " [0.14093211]\n",
      " [0.26967543]\n",
      " [0.3776157 ]\n",
      " [0.26818445]\n",
      " [0.8016856 ]\n",
      " [0.17181471]\n",
      " [0.17044804]\n",
      " [0.17908263]\n",
      " [0.20312491]\n",
      " [0.10121062]\n",
      " [0.27207145]\n",
      " [0.17262203]\n",
      " [0.38207388]\n",
      " [0.21446553]\n",
      " [0.77512693]\n",
      " [0.4168908 ]\n",
      " [0.18185821]\n",
      " [0.7713983 ]\n",
      " [0.6866143 ]\n",
      " [0.4716992 ]\n",
      " [0.08247402]\n",
      " [0.2930361 ]\n",
      " [0.205531  ]\n",
      " [0.43716016]\n",
      " [0.18160427]\n",
      " [0.08203223]\n",
      " [0.18634614]\n",
      " [0.359136  ]\n",
      " [0.6049139 ]\n",
      " [0.40714267]\n",
      " [0.25147977]\n",
      " [0.213759  ]\n",
      " [0.47925174]\n",
      " [0.20862356]\n",
      " [0.37012303]\n",
      " [0.26993427]\n",
      " [0.31755656]\n",
      " [0.4153551 ]\n",
      " [0.48478568]\n",
      " [0.52343005]\n",
      " [0.23907232]\n",
      " [0.160202  ]\n",
      " [0.13327742]\n",
      " [0.37708202]\n",
      " [0.33818987]\n",
      " [0.39966863]\n",
      " [0.74723196]\n",
      " [0.18551561]\n",
      " [0.8122524 ]\n",
      " [0.16917941]\n",
      " [0.2017037 ]\n",
      " [0.22513378]\n",
      " [0.45598662]\n",
      " [0.06046012]\n",
      " [0.6535229 ]\n",
      " [0.23336115]\n",
      " [0.1394771 ]\n",
      " [0.75837123]\n",
      " [0.5985697 ]\n",
      " [0.19638523]\n",
      " [0.17223182]\n",
      " [0.08748856]\n",
      " [0.34764028]\n",
      " [0.24784714]\n",
      " [0.2565006 ]\n",
      " [0.5266573 ]\n",
      " [0.27255332]\n",
      " [0.1626145 ]\n",
      " [0.48636973]\n",
      " [0.36936373]\n",
      " [0.22156912]\n",
      " [0.32625893]\n",
      " [0.13585973]\n",
      " [0.15823504]\n",
      " [0.44767174]\n",
      " [0.6832102 ]\n",
      " [0.48369005]\n",
      " [0.31699646]\n",
      " [0.5011805 ]\n",
      " [0.09253508]\n",
      " [0.29796642]\n",
      " [0.14859489]\n",
      " [0.54338604]\n",
      " [0.23892966]\n",
      " [0.12469903]\n",
      " [0.11453396]\n",
      " [0.20602274]\n",
      " [0.21388334]\n",
      " [0.18259323]\n",
      " [0.26339293]\n",
      " [0.22481239]\n",
      " [0.22391054]\n",
      " [0.21354514]\n",
      " [0.16687894]\n",
      " [0.28943262]\n",
      " [0.19621322]\n",
      " [0.1332902 ]\n",
      " [0.3091265 ]\n",
      " [0.44371313]\n",
      " [0.45857242]\n",
      " [0.3375667 ]\n",
      " [0.48135307]\n",
      " [0.18034002]\n",
      " [0.15418398]\n",
      " [0.7531069 ]\n",
      " [0.82684886]\n",
      " [0.3348381 ]\n",
      " [0.5367268 ]\n",
      " [0.6593876 ]\n",
      " [0.17573881]\n",
      " [0.1746614 ]\n",
      " [0.42010272]\n",
      " [0.13285562]\n",
      " [0.16094148]\n",
      " [0.38552272]\n",
      " [0.18847758]\n",
      " [0.4091822 ]\n",
      " [0.48882514]\n",
      " [0.28966853]\n",
      " [0.43213356]\n",
      " [0.6441642 ]\n",
      " [0.15868488]\n",
      " [0.25465047]\n",
      " [0.14279842]\n",
      " [0.16486198]\n",
      " [0.1804347 ]\n",
      " [0.28139913]\n",
      " [0.48825485]\n",
      " [0.25166142]\n",
      " [0.15753648]\n",
      " [0.20874229]\n",
      " [0.2204028 ]\n",
      " [0.16921225]\n",
      " [0.3864088 ]\n",
      " [0.24298996]\n",
      " [0.3218925 ]\n",
      " [0.2799627 ]\n",
      " [0.45633727]\n",
      " [0.79373443]\n",
      " [0.37016946]\n",
      " [0.24026752]\n",
      " [0.5806034 ]\n",
      " [0.34647307]\n",
      " [0.59330094]\n",
      " [0.11619288]\n",
      " [0.5079487 ]\n",
      " [0.21417132]\n",
      " [0.7234589 ]\n",
      " [0.07073984]\n",
      " [0.62978244]\n",
      " [0.2535216 ]\n",
      " [0.38360965]\n",
      " [0.21304384]\n",
      " [0.4585391 ]\n",
      " [0.5213789 ]\n",
      " [0.14306742]\n",
      " [0.12664577]\n",
      " [0.47476003]\n",
      " [0.17099771]\n",
      " [0.17151037]\n",
      " [0.17758527]\n",
      " [0.22442108]\n",
      " [0.6724764 ]\n",
      " [0.5386274 ]\n",
      " [0.2543484 ]\n",
      " [0.64808184]\n",
      " [0.12336305]\n",
      " [0.4142577 ]\n",
      " [0.15958673]\n",
      " [0.21015117]\n",
      " [0.5893353 ]\n",
      " [0.7403789 ]\n",
      " [0.31577426]\n",
      " [0.6813159 ]\n",
      " [0.17543381]\n",
      " [0.27977428]\n",
      " [0.07696813]\n",
      " [0.5392589 ]\n",
      " [0.1705119 ]\n",
      " [0.2679264 ]\n",
      " [0.23029342]\n",
      " [0.80000556]\n",
      " [0.17010441]\n",
      " [0.16311544]\n",
      " [0.22994432]\n",
      " [0.16569969]\n",
      " [0.22582579]\n",
      " [0.38973665]\n",
      " [0.14500517]\n",
      " [0.3469594 ]\n",
      " [0.16941103]\n",
      " [0.17310491]\n",
      " [0.21897534]\n",
      " [0.2512188 ]\n",
      " [0.4491445 ]\n",
      " [0.2759293 ]\n",
      " [0.17781213]\n",
      " [0.38164005]\n",
      " [0.12672836]\n",
      " [0.18183237]\n",
      " [0.29416263]\n",
      " [0.4993275 ]\n",
      " [0.18191752]\n",
      " [0.23716983]\n",
      " [0.48895574]\n",
      " [0.42260063]\n",
      " [0.49387354]\n",
      " [0.57556915]\n",
      " [0.13712844]\n",
      " [0.12164098]\n",
      " [0.24379924]\n",
      " [0.34180945]\n",
      " [0.22012964]\n",
      " [0.17880139]\n",
      " [0.46733075]\n",
      " [0.14256448]\n",
      " [0.43360847]\n",
      " [0.58561265]\n",
      " [0.22416794]\n",
      " [0.66721225]\n",
      " [0.7147983 ]\n",
      " [0.70566165]\n",
      " [0.65628564]\n",
      " [0.36382648]\n",
      " [0.21866319]\n",
      " [0.57076395]\n",
      " [0.38653284]\n",
      " [0.34186524]\n",
      " [0.61886114]\n",
      " [0.6804434 ]\n",
      " [0.13983345]\n",
      " [0.29733887]\n",
      " [0.4740355 ]\n",
      " [0.447257  ]\n",
      " [0.65207475]\n",
      " [0.6059711 ]\n",
      " [0.13119817]\n",
      " [0.2433612 ]\n",
      " [0.18405324]\n",
      " [0.07253385]\n",
      " [0.47104675]\n",
      " [0.18410942]\n",
      " [0.3077966 ]\n",
      " [0.4050892 ]\n",
      " [0.32305354]\n",
      " [0.22962406]\n",
      " [0.16841292]\n",
      " [0.3159265 ]\n",
      " [0.512986  ]\n",
      " [0.34004572]\n",
      " [0.7481892 ]\n",
      " [0.3293763 ]\n",
      " [0.53485954]\n",
      " [0.1148712 ]\n",
      " [0.5480942 ]\n",
      " [0.48410022]\n",
      " [0.10892981]\n",
      " [0.36304784]\n",
      " [0.36746222]\n",
      " [0.30183592]\n",
      " [0.44758156]\n",
      " [0.61746657]\n",
      " [0.33405882]\n",
      " [0.22331783]\n",
      " [0.2099585 ]\n",
      " [0.22404087]\n",
      " [0.27786946]\n",
      " [0.6370619 ]\n",
      " [0.21101171]\n",
      " [0.4381412 ]\n",
      " [0.38566995]\n",
      " [0.57040316]\n",
      " [0.2412126 ]\n",
      " [0.18451378]\n",
      " [0.7681586 ]\n",
      " [0.6211543 ]\n",
      " [0.39991218]\n",
      " [0.22550872]\n",
      " [0.33324325]\n",
      " [0.17351332]\n",
      " [0.23301914]\n",
      " [0.40783632]\n",
      " [0.39742762]\n",
      " [0.2458666 ]\n",
      " [0.33025056]\n",
      " [0.2579949 ]\n",
      " [0.2797367 ]\n",
      " [0.38923723]\n",
      " [0.12507385]\n",
      " [0.32060987]\n",
      " [0.306474  ]\n",
      " [0.5856164 ]\n",
      " [0.19540042]\n",
      " [0.25367492]\n",
      " [0.21249458]\n",
      " [0.22279024]\n",
      " [0.23419753]\n",
      " [0.19486874]\n",
      " [0.23130658]\n",
      " [0.65954536]\n",
      " [0.20434415]\n",
      " [0.16711384]\n",
      " [0.59387124]\n",
      " [0.8072156 ]\n",
      " [0.48013112]\n",
      " [0.4345752 ]\n",
      " [0.24472016]\n",
      " [0.6477865 ]\n",
      " [0.5379317 ]\n",
      " [0.38995713]\n",
      " [0.274523  ]\n",
      " [0.18691012]\n",
      " [0.5826885 ]\n",
      " [0.62208605]\n",
      " [0.4115922 ]\n",
      " [0.49517134]\n",
      " [0.30875325]\n",
      " [0.18161312]\n",
      " [0.7814139 ]\n",
      " [0.15536508]\n",
      " [0.7466333 ]\n",
      " [0.21584964]\n",
      " [0.51650643]\n",
      " [0.2749741 ]\n",
      " [0.295029  ]\n",
      " [0.29948217]\n",
      " [0.1400932 ]] \n",
      " Correct (Y):  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      " Accuracy:  0.75390625\n"
     ]
    }
   ],
   "source": [
    "#Launch Graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed = {X: x_data, Y: y_data}\n",
    "    \n",
    "    for step in range(20001):\n",
    "        sess.run(train, feed_dict=feed)\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict=feed))\n",
    "            \n",
    "    #Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict=feed)\n",
    "    print('\\n Hypothesis: ', h, '\\n Correct (Y): ', c, '\\n Accuracy: ', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb5e78",
   "metadata": {},
   "source": [
    "# Softmax classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abb182a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5866632\n",
      "200 0.70099527\n",
      "400 0.59057564\n",
      "600 0.49806285\n",
      "800 0.4085543\n",
      "1000 0.31897604\n",
      "1200 0.24187851\n",
      "1400 0.21791509\n",
      "1600 0.19906487\n",
      "1800 0.1830743\n",
      "2000 0.1693553\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 4])\n",
    "Y = tf.placeholder(tf.float32, [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                print(step, cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3035f41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "'''\n",
    "(101, 16) (101, 1)\n",
    "'''\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "152539fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot: Tensor(\"one_hot_1:0\", shape=(None, 1, 7), dtype=float32)\n",
      "reshape one_hot: Tensor(\"Reshape_1:0\", shape=(None, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot:\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape one_hot:\", Y_one_hot)\n",
    "\n",
    "'''\n",
    "one_hot: Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
    "reshape one_hot: Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n",
    "'''\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed819318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                                 labels=tf.stop_gradient([Y_one_hot])))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bddd7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tCost: 6.183\tAcc: 15.84%\n",
      "Step:   100\tCost: 0.725\tAcc: 86.14%\n",
      "Step:   200\tCost: 0.458\tAcc: 90.10%\n",
      "Step:   300\tCost: 0.348\tAcc: 90.10%\n",
      "Step:   400\tCost: 0.279\tAcc: 94.06%\n",
      "Step:   500\tCost: 0.232\tAcc: 94.06%\n",
      "Step:   600\tCost: 0.199\tAcc: 94.06%\n",
      "Step:   700\tCost: 0.173\tAcc: 95.05%\n",
      "Step:   800\tCost: 0.153\tAcc: 97.03%\n",
      "Step:   900\tCost: 0.138\tAcc: 98.02%\n",
      "Step:  1000\tCost: 0.124\tAcc: 98.02%\n",
      "Step:  1100\tCost: 0.114\tAcc: 98.02%\n",
      "Step:  1200\tCost: 0.104\tAcc: 98.02%\n",
      "Step:  1300\tCost: 0.096\tAcc: 99.01%\n",
      "Step:  1400\tCost: 0.089\tAcc: 99.01%\n",
      "Step:  1500\tCost: 0.083\tAcc: 99.01%\n",
      "Step:  1600\tCost: 0.078\tAcc: 99.01%\n",
      "Step:  1700\tCost: 0.073\tAcc: 99.01%\n",
      "Step:  1800\tCost: 0.069\tAcc: 100.00%\n",
      "Step:  1900\tCost: 0.065\tAcc: 100.00%\n",
      "Step:  2000\tCost: 0.062\tAcc: 100.00%\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "                                        \n",
    "        if step % 100 == 0:\n",
    "            print(\"Step: {:5}\\tCost: {:.3f}\\tAcc: {:.2%}\".format(step, cost_val, acc_val))\n",
    "\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3fc89f",
   "metadata": {},
   "source": [
    "# Datasets: tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbfc68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "795038ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c878547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.3002696 [[ 0.5666105   1.7702644   0.9448427 ]\n",
      " [-1.1833189  -1.0453693  -0.45675272]\n",
      " [ 0.82325035 -1.1157026   0.97933495]]\n",
      "1 4.3315525 [[ 0.5786585   1.807474    0.89558506]\n",
      " [-1.0541725  -0.87097436 -0.76029396]\n",
      " [ 0.9485921  -0.9285184   0.6668089 ]]\n",
      "2 3.3154461 [[ 0.52845436  1.8442533   0.90900993]\n",
      " [-1.2153902  -0.69753987 -0.77251077]\n",
      " [ 0.7773819  -0.7418711   0.6513718 ]]\n",
      "3 2.3522854 [[ 0.5107834   1.8796526   0.89128155]\n",
      " [-1.2094601  -0.5274408  -0.9485399 ]\n",
      " [ 0.76753783 -0.55724746  0.47659224]]\n",
      "4 1.6112276 [[ 0.46307632  1.9114046   0.9072366 ]\n",
      " [-1.367305   -0.36962146 -0.94851434]\n",
      " [ 0.5970584  -0.3813647   0.4711889 ]]\n",
      "5 1.4660379 [[ 0.4542857   1.9255699   0.90186185]\n",
      " [-1.328568   -0.29801154 -1.0588613 ]\n",
      " [ 0.6148558  -0.2771809   0.3492077 ]]\n",
      "6 1.4208002 [[ 0.4378761   1.9187871   0.9250542 ]\n",
      " [-1.3241907  -0.34313193 -1.0181181 ]\n",
      " [ 0.5908889  -0.27937546  0.3753692 ]]\n",
      "7 1.3989806 [[ 0.42339507  1.9175317   0.9407906 ]\n",
      " [-1.3129497  -0.35795903 -1.0145321 ]\n",
      " [ 0.5766684  -0.25396886  0.36418313]]\n",
      "8 1.377837 [[ 0.4101845   1.9151022   0.95643073]\n",
      " [-1.2959129  -0.3791405  -1.0103875 ]\n",
      " [ 0.5687696  -0.23516256  0.35327563]]\n",
      "9 1.3572627 [[ 0.39647374  1.9133621   0.9718816 ]\n",
      " [-1.282117   -0.39664087 -1.0066831 ]\n",
      " [ 0.5582135  -0.21349156  0.34216073]]\n",
      "10 1.3372327 [[ 0.3831371   1.9113506   0.98722976]\n",
      " [-1.2671143  -0.41559854 -1.0027281 ]\n",
      " [ 0.5494167  -0.19382434  0.33129025]]\n",
      "11 1.3177404 [[ 0.36972964  1.9095656   1.0024223 ]\n",
      " [-1.253199   -0.43331242 -0.9989295 ]\n",
      " [ 0.5401095  -0.17362033  0.32039344]]\n",
      "12 1.2987831 [[ 0.35649365  1.9077343   1.0174897 ]\n",
      " [-1.239145   -0.45122302 -0.9950728 ]\n",
      " [ 0.53150976 -0.15425485  0.3096277 ]]\n",
      "13 1.2803583 [[ 0.34329948  1.9060017   1.0324165 ]\n",
      " [-1.2256328  -0.4685267  -0.99128133]\n",
      " [ 0.52294576 -0.13497894  0.2989158 ]]\n",
      "14 1.2624636 [[ 0.33022466  1.9042802   1.0472127 ]\n",
      " [-1.2122782  -0.48566943 -0.9874931 ]\n",
      " [ 0.5148003  -0.11622705  0.28830937]]\n",
      "15 1.2450953 [[ 0.31722778  1.9026154   1.0618744 ]\n",
      " [-1.1993079  -0.50238305 -0.9837498 ]\n",
      " [ 0.50685036 -0.09775582  0.27778807]]\n",
      "16 1.2282493 [[ 0.3043373   1.9009748   1.0764055 ]\n",
      " [-1.186588   -0.5188188  -0.98003393]\n",
      " [ 0.49922958 -0.07971945  0.2673725 ]]\n",
      "17 1.2119207 [[ 0.2915392   1.8993728   1.0908055 ]\n",
      " [-1.1742022  -0.53487605 -0.97636247]\n",
      " [ 0.49185532 -0.06203191  0.2570592 ]]\n",
      "18 1.196104 [[ 0.27884555  1.8977953   1.1050767 ]\n",
      " [-1.162099   -0.5506098  -0.9727319 ]\n",
      " [ 0.4847784  -0.04475453  0.24685873]]\n",
      "19 1.1807921 [[ 0.2662517   1.8962463   1.1192195 ]\n",
      " [-1.1503117  -0.56597775 -0.9691512 ]\n",
      " [ 0.4779646  -0.02785474  0.23677272]]\n",
      "20 1.1659782 [[ 0.2537636   1.8947182   1.1332358 ]\n",
      " [-1.1388185  -0.58100086 -0.9656213 ]\n",
      " [ 0.47143394 -0.01135946  0.2268081 ]]\n",
      "21 1.1516536 [[ 0.24137986  1.8932114   1.1471263 ]\n",
      " [-1.127633   -0.5956594  -0.9621483 ]\n",
      " [ 0.4651701   0.00474464  0.21696785]]\n",
      "22 1.1378095 [[ 0.22910377  1.8917211   1.1608926 ]\n",
      " [-1.1167442  -0.60996187 -0.95873463]\n",
      " [ 0.45918044  0.02044501  0.20725712]]\n",
      "23 1.1244361 [[ 0.21693519  1.8902466   1.1745358 ]\n",
      " [-1.1061574  -0.6238983  -0.9553849 ]\n",
      " [ 0.45345536  0.03574818  0.19767903]]\n",
      "24 1.111523 [[ 0.20487605  1.8887844   1.1880571 ]\n",
      " [-1.0958657  -0.6374728  -0.95210207]\n",
      " [ 0.4479965   0.05064862  0.18823746]]\n",
      "25 1.0990592 [[ 0.19292651  1.8873333   1.2014577 ]\n",
      " [-1.0858703  -0.6506805  -0.94888985]\n",
      " [ 0.44279665  0.0651507   0.1789352 ]]\n",
      "26 1.0870326 [[ 0.18108776  1.8858907   1.214739  ]\n",
      " [-1.0761653  -0.6635243  -0.945751  ]\n",
      " [ 0.43785453  0.07925282  0.1697752 ]]\n",
      "27 1.0754311 [[ 0.16935988  1.8844556   1.2279019 ]\n",
      " [-1.0667496  -0.67600244 -0.94268864]\n",
      " [ 0.43316352  0.09295944  0.16075958]]\n",
      "28 1.0642421 [[ 0.15774353  1.8830259   1.240948  ]\n",
      " [-1.0576174  -0.68811804 -0.9397052 ]\n",
      " [ 0.42872068  0.10627148  0.15189041]]\n",
      "29 1.0534527 [[ 0.14623858  1.8816007   1.253878  ]\n",
      " [-1.0487661  -0.69987136 -0.93680316]\n",
      " [ 0.42451936  0.11919411  0.1431691 ]]\n",
      "30 1.0430496 [[ 0.13484529  1.8801787   1.2666934 ]\n",
      " [-1.0401899  -0.71126646 -0.9339844 ]\n",
      " [ 0.4205554   0.1317302   0.13459696]]\n",
      "31 1.0330192 [[ 0.12356333  1.8787589   1.279395  ]\n",
      " [-1.0318844  -0.7223054  -0.9312509 ]\n",
      " [ 0.41682225  0.1438857   0.12617463]]\n",
      "32 1.0233481 [[ 0.11239254  1.8773406   1.2919842 ]\n",
      " [-1.0238439  -0.7329929  -0.928604  ]\n",
      " [ 0.41331458  0.15566534  0.11790267]]\n",
      "33 1.0140226 [[ 0.1013324   1.8759229   1.304462  ]\n",
      " [-1.016063   -0.7433328  -0.926045  ]\n",
      " [ 0.41002584  0.16707559  0.10978117]]\n",
      "34 1.005029 [[ 0.0903824   1.8745055   1.3168293 ]\n",
      " [-1.0085356  -0.75333035 -0.9235748 ]\n",
      " [ 0.40695     0.1781227   0.1018099 ]]\n",
      "35 0.99635446 [[ 0.07954185  1.873088    1.3290874 ]\n",
      " [-1.0012558  -0.7629908  -0.92119414]\n",
      " [ 0.4040805   0.18881375  0.09398836]]\n",
      "36 0.98798555 [[ 0.06880998  1.87167     1.3412372 ]\n",
      " [-0.99421704 -0.7723202  -0.9189034 ]\n",
      " [ 0.401411    0.19915584  0.08631579]]\n",
      "37 0.97990906 [[ 0.05818588  1.8702515   1.3532798 ]\n",
      " [-0.9874131  -0.78132474 -0.9167028 ]\n",
      " [ 0.39893475  0.20915678  0.07879111]]\n",
      "38 0.97211254 [[ 0.04766859  1.8688323   1.3652163 ]\n",
      " [-0.9808373  -0.7900111  -0.9145922 ]\n",
      " [ 0.3966453   0.21882428  0.07141307]]\n",
      "39 0.9645835 [[ 0.03725702  1.8674126   1.3770475 ]\n",
      " [-0.9744831  -0.7983861  -0.91257143]\n",
      " [ 0.39453593  0.22816658  0.06418016]]\n",
      "40 0.9573102 [[ 0.02695006  1.8659923   1.3887748 ]\n",
      " [-0.9683436  -0.80645704 -0.9106399 ]\n",
      " [ 0.39260015  0.23719175  0.05709075]]\n",
      "41 0.9502808 [[ 0.01674646  1.8645717   1.400399  ]\n",
      " [-0.9624124  -0.81423116 -0.90879697]\n",
      " [ 0.39083135  0.24590835  0.05014298]]\n",
      "42 0.94348425 [[ 0.00664499  1.8631511   1.4119211 ]\n",
      " [-0.9566827  -0.82171607 -0.9070418 ]\n",
      " [ 0.38922322  0.25432464  0.04333484]]\n",
      "43 0.9369098 [[-0.00335569  1.8617306   1.4233422 ]\n",
      " [-0.95114785 -0.82891935 -0.90537333]\n",
      " [ 0.38776928  0.26244918  0.03666425]]\n",
      "44 0.930547 [[-0.01325693  1.8603107   1.4346634 ]\n",
      " [-0.9458013  -0.83584875 -0.9037905 ]\n",
      " [ 0.38646337  0.2702904   0.03012894]]\n",
      "45 0.924386 [[-0.02306012  1.8588916   1.4458857 ]\n",
      " [-0.94063663 -0.8425121  -0.9022919 ]\n",
      " [ 0.38529938  0.27785674  0.02372659]]\n",
      "46 0.91841733 [[-0.03276673  1.8574739   1.45701   ]\n",
      " [-0.9356474  -0.84891695 -0.9008762 ]\n",
      " [ 0.3842713   0.2851566   0.0174548 ]]\n",
      "47 0.912632 [[-0.04237817  1.8560578   1.4680376 ]\n",
      " [-0.93082744 -0.8550712  -0.899542  ]\n",
      " [ 0.38337338  0.29219824  0.01131111]]\n",
      "48 0.9070214 [[-0.05189594  1.8546437   1.4789693 ]\n",
      " [-0.92617065 -0.8609825  -0.8982875 ]\n",
      " [ 0.3825999   0.29898983  0.005293  ]]\n",
      "49 0.9015773 [[-6.1321501e-02  1.8532323e+00  1.4898064e+00]\n",
      " [-9.2167085e-01 -8.6665851e-01 -8.9711124e-01]\n",
      " [ 3.8194552e-01  3.0553934e-01 -6.0213543e-04]]\n",
      "50 0.89629185 [[-0.07065637  1.8518238   1.5005498 ]\n",
      " [-0.91732246 -0.87210673 -0.8960114 ]\n",
      " [ 0.38140476  0.31185478 -0.0063768 ]]\n",
      "51 0.89115787 [[-0.07990205  1.8504188   1.5112005 ]\n",
      " [-0.91311973 -0.8773346  -0.8949863 ]\n",
      " [ 0.38097253  0.3179438  -0.01203364]]\n",
      "52 0.88616824 [[-0.08906002  1.8490176   1.5217597 ]\n",
      " [-0.90905714 -0.88234943 -0.894034  ]\n",
      " [ 0.380644    0.323814   -0.01757528]]\n",
      "53 0.8813163 [[-0.09813181  1.8476207   1.5322284 ]\n",
      " [-0.90512943 -0.88715833 -0.89315283]\n",
      " [ 0.38041425  0.32947284 -0.02300435]]\n",
      "54 0.8765956 [[-0.10711888  1.8462286   1.5426075 ]\n",
      " [-0.90133137 -0.8917684  -0.89234084]\n",
      " [ 0.38027877  0.3349274  -0.02832342]]\n",
      "55 0.8720005 [[-0.11602274  1.8448416   1.5528984 ]\n",
      " [-0.897658   -0.8961864  -0.89159614]\n",
      " [ 0.3802331   0.34018475 -0.03353511]]\n",
      "56 0.8675251 [[-0.12484488  1.8434602   1.5631019 ]\n",
      " [-0.8941046  -0.90041906 -0.89091694]\n",
      " [ 0.38027298  0.34525177 -0.038642  ]]\n",
      "57 0.8631643 [[-0.13358672  1.8420848   1.5732191 ]\n",
      " [-0.8906663  -0.90447295 -0.8903013 ]\n",
      " [ 0.38039446  0.3501349  -0.04364662]]\n",
      "58 0.85891294 [[-0.14224976  1.8407158   1.5832511 ]\n",
      " [-0.8873389  -0.9083543  -0.8897474 ]\n",
      " [ 0.38059342  0.35484087 -0.04855152]]\n",
      "59 0.85476625 [[-0.1508354   1.8393536   1.593199  ]\n",
      " [-0.88411784 -0.91206944 -0.88925326]\n",
      " [ 0.3808663   0.35937563 -0.05335917]]\n",
      "60 0.8507197 [[-0.15934506  1.8379985   1.6030637 ]\n",
      " [-0.88099915 -0.9156242  -0.8888172 ]\n",
      " [ 0.38120943  0.36374542 -0.05807209]]\n",
      "61 0.84676903 [[-0.16778013  1.836651    1.6128464 ]\n",
      " [-0.8779787  -0.9190246  -0.8884373 ]\n",
      " [ 0.38161945  0.36795592 -0.06269263]]\n",
      "62 0.8429103 [[-0.17614202  1.8353113   1.622548  ]\n",
      " [-0.8750528  -0.922276   -0.8881117 ]\n",
      " [ 0.38209298  0.372013   -0.06722321]]\n",
      "63 0.8391396 [[-0.18443203  1.8339797   1.6321696 ]\n",
      " [-0.87221754 -0.9253842  -0.8878387 ]\n",
      " [ 0.38262713  0.37592182 -0.07166618]]\n",
      "64 0.83545345 [[-0.19265157  1.8326566   1.6417122 ]\n",
      " [-0.8694697  -0.92835414 -0.88761663]\n",
      " [ 0.38321856  0.37968808 -0.07602386]]\n",
      "65 0.83184814 [[-0.20080186  1.8313422   1.6511769 ]\n",
      " [-0.86680555 -0.9311914  -0.8874436 ]\n",
      " [ 0.38386482  0.3833164  -0.08029844]]\n",
      "66 0.8283208 [[-0.2088843   1.8300369   1.6605647 ]\n",
      " [-0.8642223  -0.9339003  -0.887318  ]\n",
      " [ 0.38456273  0.38681227 -0.08449224]]\n",
      "67 0.82486826 [[-0.21690005  1.8287408   1.6698765 ]\n",
      " [-0.8617162  -0.93648624 -0.8872382 ]\n",
      " [ 0.3853102   0.3901799  -0.08860736]]\n",
      "68 0.82148755 [[-0.22485045  1.8274543   1.6791133 ]\n",
      " [-0.8592848  -0.93895334 -0.8872025 ]\n",
      " [ 0.3861043   0.3934244  -0.09264593]]\n",
      "69 0.81817603 [[-0.23273662  1.8261776   1.6882762 ]\n",
      " [-0.85692483 -0.9413065  -0.88720936]\n",
      " [ 0.3869431   0.3965497  -0.09661005]]\n",
      "70 0.8149309 [[-0.24055985  1.8249109   1.6973661 ]\n",
      " [-0.8546339  -0.9435496  -0.88725716]\n",
      " [ 0.3878239   0.39956054 -0.1005017 ]]\n",
      "71 0.81175005 [[-0.24832124  1.8236544   1.706384  ]\n",
      " [-0.852409   -0.9456872  -0.8873445 ]\n",
      " [ 0.38874507  0.40246058 -0.10432293]]\n",
      "72 0.8086308 [[-0.256022    1.8224083   1.7153307 ]\n",
      " [-0.8502481  -0.94772285 -0.8874697 ]\n",
      " [ 0.38970402  0.4052543  -0.10807559]]\n",
      "73 0.8055711 [[-0.2636632   1.8211728   1.7242074 ]\n",
      " [-0.8481482  -0.94966096 -0.88763154]\n",
      " [ 0.39069936  0.40794504 -0.11176165]]\n",
      "74 0.8025688 [[-0.27124602  1.8199481   1.733015  ]\n",
      " [-0.8461075  -0.95150477 -0.88782847]\n",
      " [ 0.3917288   0.41053692 -0.11538295]]\n",
      "75 0.799622 [[-0.27877146  1.8187343   1.7417542 ]\n",
      " [-0.84412336 -0.9532582  -0.8880592 ]\n",
      " [ 0.39279088  0.41303313 -0.11894126]]\n",
      "76 0.7967286 [[-0.28624064  1.8175316   1.750426  ]\n",
      " [-0.84219396 -0.95492446 -0.8883223 ]\n",
      " [ 0.3938837   0.41543737 -0.12243833]]\n",
      "77 0.793887 [[-0.29365453  1.81634     1.7590315 ]\n",
      " [-0.840317   -0.95650715 -0.88861656]\n",
      " [ 0.3950059   0.41775274 -0.12587589]]\n",
      "78 0.7910954 [[-0.3010142   1.8151597   1.7675714 ]\n",
      " [-0.83849084 -0.9580092  -0.8889407 ]\n",
      " [ 0.39615574  0.41998264 -0.12925561]]\n",
      "79 0.78835225 [[-0.30832064  1.8139908   1.7760468 ]\n",
      " [-0.8367133  -0.959434   -0.88929343]\n",
      " [ 0.39733198  0.4221299  -0.13257907]]\n",
      "80 0.7856556 [[-0.31557482  1.8128335   1.7844583 ]\n",
      " [-0.83498293 -0.96078414 -0.88967365]\n",
      " [ 0.398533    0.42419773 -0.13584791]]\n",
      "81 0.7830045 [[-0.32277763  1.8116878   1.7928069 ]\n",
      " [-0.8332976  -0.96206295 -0.89008015]\n",
      " [ 0.3997578   0.42618868 -0.13906364]]\n",
      "82 0.78039724 [[-0.32993007  1.8105538   1.8010933 ]\n",
      " [-0.83165604 -0.9632728  -0.8905118 ]\n",
      " [ 0.40100482  0.4281058  -0.14222777]]\n",
      "83 0.7778326 [[-0.337033    1.8094316   1.8093185 ]\n",
      " [-0.8300564  -0.9644167  -0.89096755]\n",
      " [ 0.40227315  0.42995146 -0.14534172]]\n",
      "84 0.7753091 [[-0.34408736  1.8083212   1.8174833 ]\n",
      " [-0.82849747 -0.96549684 -0.8914463 ]\n",
      " [ 0.40356138  0.43172845 -0.14840695]]\n",
      "85 0.7728258 [[-0.35109392  1.8072227   1.8255883 ]\n",
      " [-0.82697743 -0.96651614 -0.89194703]\n",
      " [ 0.4048688   0.4334389  -0.15142483]]\n",
      "86 0.7703813 [[-0.35805362  1.8061363   1.8336346 ]\n",
      " [-0.82549536 -0.9674764  -0.8924688 ]\n",
      " [ 0.40619397  0.43508562 -0.1543967 ]]\n",
      "87 0.7679744 [[-0.36496723  1.8050617   1.8416228 ]\n",
      " [-0.8240494  -0.9683806  -0.8930106 ]\n",
      " [ 0.4075364   0.43667036 -0.15732385]]\n",
      "88 0.7656043 [[-0.37183562  1.8039992   1.8495537 ]\n",
      " [-0.8226388  -0.96923023 -0.8935715 ]\n",
      " [ 0.40889463  0.43819582 -0.16020755]]\n",
      "89 0.76326966 [[-0.37865952  1.8029487   1.8574281 ]\n",
      " [-0.8212619  -0.97002804 -0.8941506 ]\n",
      " [ 0.41026828  0.43966368 -0.16304906]]\n",
      "90 0.76096976 [[-0.38543975  1.8019104   1.8652467 ]\n",
      " [-0.81991786 -0.9707756  -0.8947471 ]\n",
      " [ 0.41165614  0.44107634 -0.16584955]]\n",
      "91 0.75870353 [[-0.39217702  1.8008841   1.8730102 ]\n",
      " [-0.81860536 -0.97147524 -0.89536   ]\n",
      " [ 0.41305768  0.4424354  -0.16861019]]\n",
      "92 0.75647 [[-0.3988721   1.79987     1.8807194 ]\n",
      " [-0.8173235  -0.9721285  -0.8959886 ]\n",
      " [ 0.4144719   0.4437431  -0.17133212]]\n",
      "93 0.7542686 [[-0.4055257   1.798868    1.888375  ]\n",
      " [-0.81607103 -0.9727375  -0.896632  ]\n",
      " [ 0.41589832  0.445001   -0.17401643]]\n",
      "94 0.7520981 [[-0.41213855  1.797878    1.8959777 ]\n",
      " [-0.8148472  -0.9733038  -0.8972896 ]\n",
      " [ 0.4173361   0.44621098 -0.17666422]]\n",
      "95 0.7499578 [[-0.4187113   1.7969003   1.9035283 ]\n",
      " [-0.8136508  -0.9738292  -0.89796054]\n",
      " [ 0.41878474  0.44737464 -0.1792765 ]]\n",
      "96 0.747847 [[-0.42524466  1.7959346   1.9110274 ]\n",
      " [-0.81248116 -0.9743152  -0.89864415]\n",
      " [ 0.4202434   0.44849375 -0.18185426]]\n",
      "97 0.7457651 [[-0.43173924  1.7949809   1.9184756 ]\n",
      " [-0.81133723 -0.97476363 -0.8993397 ]\n",
      " [ 0.4217117   0.4495697  -0.1843985 ]]\n",
      "98 0.7437111 [[-0.43819574  1.7940394   1.9258736 ]\n",
      " [-0.8102184  -0.9751756  -0.9000465 ]\n",
      " [ 0.42318887  0.45060423 -0.18691018]]\n",
      "99 0.74168444 [[-0.44461474  1.7931099   1.9332222 ]\n",
      " [-0.8091235  -0.975553   -0.90076405]\n",
      " [ 0.42467463  0.4515985  -0.18939021]]\n",
      "100 0.73968446 [[-0.4509969   1.7921925   1.9405218 ]\n",
      " [-0.8080521  -0.97589684 -0.9014916 ]\n",
      " [ 0.4261682   0.4525542  -0.19183949]]\n",
      "101 0.7377106 [[-0.4573428   1.791287    1.9477732 ]\n",
      " [-0.8070033  -0.9762087  -0.90222853]\n",
      " [ 0.4276693   0.4534725  -0.19425887]]\n",
      "102 0.7357622 [[-0.46365303  1.7903935   1.9549769 ]\n",
      " [-0.8059765  -0.9764897  -0.9029743 ]\n",
      " [ 0.4291773   0.45435482 -0.19664922]]\n",
      "103 0.7338384 [[-0.46992815  1.7895119   1.9621336 ]\n",
      " [-0.8049708  -0.9767414  -0.90372837]\n",
      " [ 0.43069202  0.45520225 -0.19901136]]\n",
      "104 0.73193896 [[-0.47616875  1.7886423   1.9692439 ]\n",
      " [-0.80398583 -0.97696453 -0.9044902 ]\n",
      " [ 0.43221274  0.45601627 -0.20134607]]\n",
      "105 0.7300632 [[-0.48237532  1.7877845   1.9763083 ]\n",
      " [-0.80302066 -0.9771607  -0.9052592 ]\n",
      " [ 0.4337393   0.45679775 -0.20365411]]\n",
      "106 0.7282106 [[-0.4885485   1.7869384   1.9833275 ]\n",
      " [-0.80207497 -0.9773307  -0.9060349 ]\n",
      " [ 0.4352711   0.45754808 -0.20593624]]\n",
      "107 0.7263808 [[-0.4946887   1.7861042   1.990302  ]\n",
      " [-0.80114794 -0.9774758  -0.90681684]\n",
      " [ 0.43680805  0.4582681  -0.20819321]]\n",
      "108 0.724573 [[-0.5007965   1.7852817   1.9972323 ]\n",
      " [-0.8002392  -0.9775969  -0.9076046 ]\n",
      " [ 0.43834955  0.45895913 -0.21042572]]\n",
      "109 0.722787 [[-0.50687236  1.7844707   2.0041192 ]\n",
      " [-0.799348   -0.97769505 -0.90839756]\n",
      " [ 0.43989548  0.4596219  -0.21263443]]\n",
      "110 0.72102195 [[-0.5129168   1.7836714   2.010963  ]\n",
      " [-0.798474   -0.9777711  -0.9091954 ]\n",
      " [ 0.44144535  0.46025762 -0.21482003]]\n",
      "111 0.719278 [[-0.5189303   1.7828836   2.0177643 ]\n",
      " [-0.7976166  -0.9778262  -0.90999776]\n",
      " [ 0.44299904  0.46086702 -0.21698312]]\n",
      "112 0.71755403 [[-0.5249134   1.7821074   2.0245237 ]\n",
      " [-0.79677546 -0.977861   -0.91080415]\n",
      " [ 0.4445561   0.46145123 -0.21912439]]\n",
      "113 0.7158501 [[-0.5308664   1.7813425   2.0312417 ]\n",
      " [-0.79594994 -0.9778765  -0.9116142 ]\n",
      " [ 0.44611642  0.46201092 -0.2212444 ]]\n",
      "114 0.71416557 [[-0.5367899   1.780589    2.0379186 ]\n",
      " [-0.79513973 -0.97787344 -0.9124275 ]\n",
      " [ 0.4476796   0.46254706 -0.22334372]]\n",
      "115 0.7125002 [[-0.54268426  1.7798468   2.0445552 ]\n",
      " [-0.7943443  -0.9778526  -0.9132437 ]\n",
      " [ 0.4492455   0.46306044 -0.22542296]]\n",
      "116 0.7108534 [[-0.5485499   1.7791158   2.0511518 ]\n",
      " [-0.7935633  -0.9778148  -0.9140625 ]\n",
      " [ 0.4508139   0.4635518  -0.22748269]]\n",
      "117 0.70922494 [[-0.5543873   1.778396    2.057709  ]\n",
      " [-0.7927963  -0.9777607  -0.91488355]\n",
      " [ 0.45238447  0.46402192 -0.2295234 ]]\n",
      "118 0.7076144 [[-0.5601968   1.7776873   2.064227  ]\n",
      " [-0.7920429  -0.9776911  -0.9157065 ]\n",
      " [ 0.45395714  0.46447146 -0.23154563]]\n",
      "119 0.70602155 [[-0.5659789   1.7769897   2.0707068 ]\n",
      " [-0.79130286 -0.97760653 -0.9165311 ]\n",
      " [ 0.45553157  0.4649013  -0.23354988]]\n",
      "120 0.70444584 [[-0.5717339   1.776303    2.0771484 ]\n",
      " [-0.7905757  -0.9775079  -0.91735697]\n",
      " [ 0.45710772  0.46531188 -0.23553663]]\n",
      "121 0.70288706 [[-0.57746226  1.7756273   2.0835526 ]\n",
      " [-0.78986114 -0.97739553 -0.91818386]\n",
      " [ 0.45868525  0.46570408 -0.23750637]]\n",
      "122 0.70134497 [[-0.5831643   1.7749623   2.0899196 ]\n",
      " [-0.7891588  -0.9772702  -0.91901153]\n",
      " [ 0.46026412  0.4660784  -0.23945956]]\n",
      "123 0.699819 [[-0.58884037  1.7743081   2.0962498 ]\n",
      " [-0.7884684  -0.97713244 -0.91983974]\n",
      " [ 0.46184415  0.46643546 -0.24139667]]\n",
      "124 0.69830906 [[-0.59449095  1.7736646   2.1025438 ]\n",
      " [-0.78778964 -0.97698283 -0.9206681 ]\n",
      " [ 0.46342513  0.4667759  -0.24331808]]\n",
      "125 0.6968147 [[-0.6001163   1.7730317   2.108802  ]\n",
      " [-0.7871222  -0.9768219  -0.9214965 ]\n",
      " [ 0.46500695  0.46710023 -0.24522422]]\n",
      "126 0.6953358 [[-0.6057168   1.7724093   2.1150248 ]\n",
      " [-0.78646576 -0.9766501  -0.92232466]\n",
      " [ 0.46658948  0.467409   -0.24711552]]\n",
      "127 0.693872 [[-0.6112928   1.7717974   2.1212127 ]\n",
      " [-0.78582007 -0.9764681  -0.9231524 ]\n",
      " [ 0.46817258  0.46770275 -0.24899237]]\n",
      "128 0.69242287 [[-0.6168446   1.7711959   2.127366  ]\n",
      " [-0.785185   -0.9762761  -0.92397946]\n",
      " [ 0.46975604  0.46798208 -0.25085515]]\n",
      "129 0.6909883 [[-0.6223725   1.7706047   2.133485  ]\n",
      " [-0.78455997 -0.97607493 -0.92480564]\n",
      " [ 0.47133994  0.4682472  -0.2527042 ]]\n",
      "130 0.6895679 [[-0.62787694  1.7700238   2.1395705 ]\n",
      " [-0.78394514 -0.97586465 -0.92563075]\n",
      " [ 0.47292387  0.46849898 -0.2545399 ]]\n",
      "131 0.68816173 [[-0.63335806  1.769453    2.1456225 ]\n",
      " [-0.7833399  -0.975646   -0.9264546 ]\n",
      " [ 0.47450808  0.46873745 -0.25636262]]\n",
      "132 0.68676925 [[-0.6388163   1.7688923   2.1516414 ]\n",
      " [-0.7827444  -0.97541916 -0.92727697]\n",
      " [ 0.47609213  0.46896344 -0.25817266]]\n",
      "133 0.68539035 [[-0.64425194  1.7683415   2.1576278 ]\n",
      " [-0.782158   -0.97518474 -0.9280978 ]\n",
      " [ 0.4776762   0.46917707 -0.2599704 ]]\n",
      "134 0.6840247 [[-0.6496653   1.7678008   2.1635818 ]\n",
      " [-0.78158087 -0.97494286 -0.9289168 ]\n",
      " [ 0.47925997  0.469379   -0.2617561 ]]\n",
      "135 0.682672 [[-0.6550566   1.7672698   2.1695042 ]\n",
      " [-0.7810125  -0.9746942  -0.9297339 ]\n",
      " [ 0.48084357  0.4695694  -0.26353008]]\n",
      "136 0.68133223 [[-0.66042614  1.7667487   2.1753948 ]\n",
      " [-0.7804529  -0.9744387  -0.9305489 ]\n",
      " [ 0.48242667  0.4697489  -0.26529264]]\n",
      "137 0.6800051 [[-0.6657742   1.7662371   2.1812544 ]\n",
      " [-0.7799016  -0.9741772  -0.9313617 ]\n",
      " [ 0.48400947  0.46991757 -0.2670441 ]]\n",
      "138 0.6786904 [[-0.6711011   1.7657353   2.187083  ]\n",
      " [-0.7793588  -0.97390956 -0.9321721 ]\n",
      " [ 0.4855916   0.47007608 -0.26878473]]\n",
      "139 0.6773877 [[-0.67640704  1.7652429   2.1928813 ]\n",
      " [-0.77882403 -0.9736364  -0.93298006]\n",
      " [ 0.4871732   0.47022453 -0.2705148 ]]\n",
      "140 0.6760971 [[-0.68169236  1.7647601   2.1986494 ]\n",
      " [-0.77829725 -0.97335786 -0.9337854 ]\n",
      " [ 0.4887541   0.47036335 -0.2722345 ]]\n",
      "141 0.67481834 [[-0.68695724  1.7642866   2.2043877 ]\n",
      " [-0.7777782  -0.9730743  -0.93458796]\n",
      " [ 0.4903343   0.4704928  -0.27394417]]\n",
      "142 0.6735511 [[-0.692202    1.7638224   2.2100966 ]\n",
      " [-0.7772668  -0.97278595 -0.9353877 ]\n",
      " [ 0.4919137   0.4706133  -0.27564403]]\n",
      "143 0.67229515 [[-0.6974268   1.7633675   2.2157764 ]\n",
      " [-0.7767628  -0.9724932  -0.93618447]\n",
      " [ 0.49349234  0.470725   -0.27733436]]\n",
      "144 0.67105055 [[-0.702632    1.7629218   2.2214274 ]\n",
      " [-0.77626616 -0.97219604 -0.9369782 ]\n",
      " [ 0.49506995  0.47082835 -0.27901533]]\n",
      "145 0.66981685 [[-0.70781773  1.7624851   2.2270498 ]\n",
      " [-0.77577657 -0.97189504 -0.9377688 ]\n",
      " [ 0.49664673  0.47092342 -0.2806872 ]]\n",
      "146 0.6685942 [[-0.7129843   1.7620574   2.232644  ]\n",
      " [-0.7752941  -0.97159016 -0.93855613]\n",
      " [ 0.49822238  0.47101074 -0.28235018]]\n",
      "147 0.6673821 [[-0.7181319   1.7616386   2.2382104 ]\n",
      " [-0.77481836 -0.9712819  -0.9393402 ]\n",
      " [ 0.4997971   0.47109032 -0.28400448]]\n",
      "148 0.6661805 [[-0.7232608   1.7612287   2.2437491 ]\n",
      " [-0.77434945 -0.97097015 -0.9401208 ]\n",
      " [ 0.5013706   0.47116262 -0.28565028]]\n",
      "149 0.66498923 [[-0.7283712   1.7608275   2.2492607 ]\n",
      " [-0.773887   -0.97065544 -0.94089794]\n",
      " [ 0.50294316  0.47122762 -0.2872878 ]]\n",
      "150 0.66380805 [[-0.7334633   1.7604351   2.2547452 ]\n",
      " [-0.7734312  -0.9703377  -0.94167155]\n",
      " [ 0.50451434  0.47128588 -0.28891724]]\n",
      "151 0.662637 [[-0.73853725  1.7600513   2.2602031 ]\n",
      " [-0.7729815  -0.97001743 -0.9424415 ]\n",
      " [ 0.5060845   0.47133726 -0.2905388 ]]\n",
      "152 0.6614758 [[-0.7435934   1.759676    2.2656345 ]\n",
      " [-0.77253824 -0.96969444 -0.9432078 ]\n",
      " [ 0.50765324  0.47138238 -0.2921526 ]]\n",
      "153 0.6603243 [[-0.74863183  1.7593092   2.2710397 ]\n",
      " [-0.7721009  -0.9693693  -0.9439703 ]\n",
      " [ 0.50922084  0.47142103 -0.2937589 ]]\n",
      "154 0.6591823 [[-0.7536529   1.7589507   2.2764192 ]\n",
      " [-0.7716696  -0.9690418  -0.94472903]\n",
      " [ 0.51078707  0.47145373 -0.29535782]]\n",
      "155 0.6580497 [[-0.7586566   1.7586006   2.281773  ]\n",
      " [-0.7712442  -0.9687124  -0.94548386]\n",
      " [ 0.512352    0.47148055 -0.29694954]]\n",
      "156 0.6569264 [[-0.7636433   1.7582587   2.2871017 ]\n",
      " [-0.7708246  -0.96838105 -0.9462348 ]\n",
      " [ 0.5139155   0.4715017  -0.29853418]]\n",
      "157 0.6558122 [[-0.76861316  1.757925    2.2924051 ]\n",
      " [-0.7704106  -0.9680481  -0.9469818 ]\n",
      " [ 0.51547766  0.47151726 -0.30011195]]\n",
      "158 0.6547071 [[-0.7735663   1.7575995   2.2976837 ]\n",
      " [-0.7700021  -0.96771353 -0.94772476]\n",
      " [ 0.5170384   0.47152755 -0.30168298]]\n",
      "159 0.6536106 [[-0.77850294  1.7572819   2.302938  ]\n",
      " [-0.7695992  -0.9673775  -0.9484637 ]\n",
      " [ 0.5185976   0.47153273 -0.3032474 ]]\n",
      "160 0.65252304 [[-0.78342324  1.7569723   2.308168  ]\n",
      " [-0.7692015  -0.96704036 -0.94919854]\n",
      " [ 0.5201555   0.4715328  -0.30480534]]\n",
      "161 0.6514439 [[-0.78832746  1.7566706   2.3133738 ]\n",
      " [-0.7688092  -0.9667019  -0.9499293 ]\n",
      " [ 0.5217118   0.47152814 -0.306357  ]]\n",
      "162 0.65037334 [[-0.7932157   1.7563767   2.3185558 ]\n",
      " [-0.76842207 -0.9663625  -0.9506559 ]\n",
      " [ 0.5232666   0.47151875 -0.30790243]]\n",
      "163 0.649311 [[-0.79808813  1.7560906   2.3237145 ]\n",
      " [-0.76803994 -0.96602213 -0.9513783 ]\n",
      " [ 0.52481997  0.47150478 -0.30944183]]\n",
      "164 0.64825714 [[-0.80294496  1.7558122   2.3288498 ]\n",
      " [-0.7676629  -0.96568096 -0.9520965 ]\n",
      " [ 0.5263717   0.47148645 -0.31097528]]\n",
      "165 0.6472111 [[-0.8077863   1.7555413   2.333962  ]\n",
      " [-0.7672907  -0.9653391  -0.9528105 ]\n",
      " [ 0.527922    0.47146383 -0.31250292]]\n",
      "166 0.6461732 [[-0.81261235  1.755278    2.3390515 ]\n",
      " [-0.7669234  -0.9649967  -0.95352024]\n",
      " [ 0.5294707   0.47143707 -0.31402484]]\n",
      "167 0.6451429 [[-0.8174233   1.755022    2.3441184 ]\n",
      " [-0.7665609  -0.96465373 -0.9542257 ]\n",
      " [ 0.5310178   0.47140634 -0.3155412 ]]\n",
      "168 0.6441207 [[-0.82221925  1.7547735   2.3491628 ]\n",
      " [-0.76620305 -0.96431035 -0.95492697]\n",
      " [ 0.53256327  0.47137174 -0.3170521 ]]\n",
      "169 0.643106 [[-0.8270004   1.7545323   2.354185  ]\n",
      " [-0.76584977 -0.9639667  -0.95562387]\n",
      " [ 0.53410715  0.47133332 -0.3185576 ]]\n",
      "170 0.64209867 [[-0.83176684  1.7542984   2.3591855 ]\n",
      " [-0.765501   -0.96362287 -0.9563165 ]\n",
      " [ 0.5356494   0.47129127 -0.3200578 ]]\n",
      "171 0.64109886 [[-0.8365188   1.7540717   2.364164  ]\n",
      " [-0.7651568  -0.96327883 -0.9570047 ]\n",
      " [ 0.53719     0.47124574 -0.32155287]]\n",
      "172 0.64010644 [[-0.84125644  1.7538521   2.3691213 ]\n",
      " [-0.76481694 -0.96293473 -0.9576887 ]\n",
      " [ 0.538729    0.4711967  -0.32304287]]\n",
      "173 0.63912123 [[-0.8459798   1.7536396   2.3740573 ]\n",
      " [-0.76448137 -0.9625907  -0.9583683 ]\n",
      " [ 0.5402664   0.47114435 -0.32452792]]\n",
      "174 0.6381431 [[-0.8506892   1.7534341   2.3789723 ]\n",
      " [-0.7641501  -0.9622467  -0.95904356]\n",
      " [ 0.5418021   0.47108877 -0.32600805]]\n",
      "175 0.637172 [[-0.8553846   1.7532355   2.3838663 ]\n",
      " [-0.763823   -0.9619029  -0.9597145 ]\n",
      " [ 0.54333615  0.4710301  -0.32748342]]\n",
      "176 0.6362078 [[-0.86006624  1.7530437   2.3887398 ]\n",
      " [-0.7635     -0.96155936 -0.96038103]\n",
      " [ 0.5448685   0.47096834 -0.32895404]]\n",
      "177 0.63525045 [[-0.86473423  1.7528586   2.3935928 ]\n",
      " [-0.7631811  -0.961216   -0.96104324]\n",
      " [ 0.54639924  0.4709037  -0.33042008]]\n",
      "178 0.6342999 [[-0.8693887   1.7526804   2.3984256 ]\n",
      " [-0.76286614 -0.96087307 -0.9617011 ]\n",
      " [ 0.5479283   0.4708361  -0.33188155]]\n",
      "179 0.6333559 [[-0.87402976  1.7525089   2.4032383 ]\n",
      " [-0.76255524 -0.96053046 -0.9623546 ]\n",
      " [ 0.5494556   0.4707658  -0.33333856]]\n",
      "180 0.63241845 [[-0.8786576   1.7523439   2.408031  ]\n",
      " [-0.7622481  -0.9601884  -0.96300375]\n",
      " [ 0.5509813   0.47069272 -0.33479118]]\n",
      "181 0.6314875 [[-0.8832723   1.7521855   2.4128041 ]\n",
      " [-0.7619449  -0.9598468  -0.96364856]\n",
      " [ 0.55250525  0.47061706 -0.3362395 ]]\n",
      "182 0.6305629 [[-0.887874    1.7520335   2.4175577 ]\n",
      " [-0.76164544 -0.95950574 -0.96428907]\n",
      " [ 0.5540275   0.47053888 -0.33768356]]\n",
      "183 0.62964463 [[-0.8924628   1.7518879   2.422292  ]\n",
      " [-0.7613497  -0.95916533 -0.9649252 ]\n",
      " [ 0.5555481   0.4704582  -0.33912346]]\n",
      "184 0.62873256 [[-0.8970389   1.7517488   2.4270072 ]\n",
      " [-0.7610576  -0.9588255  -0.96555704]\n",
      " [ 0.5570669   0.47037515 -0.34055924]]\n",
      "185 0.6278266 [[-0.9016023   1.7516159   2.4317036 ]\n",
      " [-0.7607691  -0.9584865  -0.96618456]\n",
      " [ 0.5585841   0.4702897  -0.34199098]]\n",
      "186 0.62692666 [[-0.90615326  1.7514893   2.436381  ]\n",
      " [-0.7604842  -0.9581481  -0.9668078 ]\n",
      " [ 0.56009954  0.47020206 -0.34341878]]\n",
      "187 0.6260327 [[-0.9106918   1.7513689   2.44104   ]\n",
      " [-0.7602029  -0.9578106  -0.96742666]\n",
      " [ 0.56161326  0.4701122  -0.3448426 ]]\n",
      "188 0.62514466 [[-0.91521806  1.7512546   2.4456806 ]\n",
      " [-0.75992495 -0.9574739  -0.9680413 ]\n",
      " [ 0.5631253   0.47002015 -0.34626263]]\n",
      "189 0.6242625 [[-0.91973215  1.7511463   2.450303  ]\n",
      " [-0.7596505  -0.95713794 -0.96865165]\n",
      " [ 0.5646356   0.4699261  -0.34767884]]\n",
      "190 0.623386 [[-0.92423415  1.751044    2.4549074 ]\n",
      " [-0.7593793  -0.956803   -0.9692578 ]\n",
      " [ 0.5661442   0.46982992 -0.34909132]]\n",
      "191 0.6225152 [[-0.9287243   1.7509477   2.4594939 ]\n",
      " [-0.75911164 -0.9564688  -0.96985966]\n",
      " [ 0.56765103  0.46973193 -0.35050014]]\n",
      "192 0.62165004 [[-0.93320256  1.7508572   2.4640625 ]\n",
      " [-0.758847   -0.9561358  -0.9704573 ]\n",
      " [ 0.5691563   0.4696318  -0.3519053 ]]\n",
      "193 0.62079036 [[-0.93766916  1.7507727   2.4686136 ]\n",
      " [-0.7585859  -0.9558035  -0.97105074]\n",
      " [ 0.57065964  0.46953008 -0.3533069 ]]\n",
      "194 0.6199361 [[-0.94212407  1.7506939   2.4731474 ]\n",
      " [-0.75832766 -0.95547247 -0.97164   ]\n",
      " [ 0.5721615   0.46942627 -0.35470495]]\n",
      "195 0.61908716 [[-0.9465675   1.7506208   2.477664  ]\n",
      " [-0.75807285 -0.95514214 -0.97222507]\n",
      " [ 0.5736614   0.46932095 -0.35609952]]\n",
      "196 0.61824363 [[-0.95099944  1.7505534   2.4821634 ]\n",
      " [-0.7578209  -0.9548131  -0.972806  ]\n",
      " [ 0.5751598   0.46921366 -0.35749066]]\n",
      "197 0.61740553 [[-0.95542014  1.7504916   2.486646  ]\n",
      " [-0.7575723  -0.95448494 -0.9733828 ]\n",
      " [ 0.5766562   0.46910498 -0.35887843]]\n",
      "198 0.61657226 [[-0.95982957  1.7504354   2.4911118 ]\n",
      " [-0.7573264  -0.9541581  -0.97395545]\n",
      " [ 0.5781513   0.46899435 -0.36026284]]\n",
      "199 0.6157444 [[-0.964228    1.7503847   2.495561  ]\n",
      " [-0.7570839  -0.9538321  -0.974524  ]\n",
      " [ 0.57964426  0.46888247 -0.36164397]]\n",
      "200 0.61492145 [[-0.9686153   1.7503394   2.4999936 ]\n",
      " [-0.7568439  -0.9535076  -0.9750885 ]\n",
      " [ 0.5811359   0.46876866 -0.36302185]]\n",
      "Prediction: [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db7a6d",
   "metadata": {},
   "source": [
    "## Too big learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01589995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.503568 [[ 2.1473308  -1.8104824   0.9891026 ]\n",
      " [ 1.4515973  -0.7766717  -1.526612  ]\n",
      " [ 0.7292234   0.15615118 -0.7374763 ]]\n",
      "1 20.505472 [[ 1.0224875  -1.2480373   1.5515008 ]\n",
      " [-2.6730843   1.8482146  -0.02681684]\n",
      " [-3.3955927   2.9685717   0.5749195 ]]\n",
      "2 21.695984 [[ 1.3974874  -2.176369    2.1048326 ]\n",
      " [-0.2355845  -2.0708146   1.4547127 ]\n",
      " [-0.9580927  -0.77212095  1.8781126 ]]\n",
      "3 12.32185 [[ 1.7724773 -1.613869   1.1673429]\n",
      " [ 2.201895   0.5541854 -3.6077666]\n",
      " [ 1.479397   2.040379  -3.3718774]]\n",
      "4 inf [[ 0.6637833 -1.067675   1.7298429]\n",
      " [-1.8880174  3.144098  -2.1077666]\n",
      " [-2.572727   4.7800026 -2.0593774]]\n",
      "5 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "6 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "7 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "8 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "9 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "10 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "11 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "12 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "13 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "14 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "15 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "16 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "17 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "18 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "19 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "20 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "21 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "22 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "23 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "24 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "25 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "26 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "27 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "28 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "29 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "30 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "31 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "32 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "33 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "34 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "35 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "36 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "37 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "38 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "39 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "40 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "41 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "42 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "43 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "44 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "45 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "46 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "47 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "48 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "49 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "50 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "51 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "52 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "53 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "54 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "55 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "56 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "57 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "58 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "59 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "60 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "61 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "62 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "63 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "64 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "65 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "66 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "67 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "68 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "69 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "70 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "71 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "72 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "73 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "74 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "75 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "76 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "77 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "78 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "79 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "80 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "81 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "82 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "83 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "84 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "85 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "86 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "87 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "88 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "89 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "90 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "91 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "92 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "93 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "94 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "95 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "96 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "97 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "98 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "99 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "100 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "101 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "102 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "103 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "104 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "105 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "106 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "107 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "108 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "109 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "110 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "111 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "112 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "113 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "114 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "115 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "116 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "117 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "118 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "119 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "120 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "121 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "122 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "123 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "124 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "125 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "126 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "127 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "128 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "129 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "130 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "131 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "132 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "133 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "134 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "135 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "136 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "137 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "138 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "139 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "140 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "141 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "142 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "143 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "144 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "145 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "146 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "147 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "148 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "149 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "150 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "151 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "152 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "153 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "154 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "155 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "156 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "157 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "158 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "159 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "160 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "161 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "162 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "163 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "164 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "165 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "166 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "167 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "168 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "169 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "170 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "171 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "172 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "173 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "174 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "175 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "176 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "177 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "178 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "179 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "180 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "181 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "182 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "183 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "184 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "185 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "186 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "187 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "188 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "189 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "190 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "191 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "192 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "193 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "194 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "195 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "196 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "197 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "198 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "199 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "200 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "Prediction: [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.5).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ebd320",
   "metadata": {},
   "source": [
    "## Too small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0463261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "1 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "2 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "3 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "4 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "5 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "6 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "7 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "8 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "9 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "10 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "11 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "12 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "13 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "14 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "15 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "16 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "17 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "18 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "19 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "20 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "21 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "22 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "23 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "24 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "25 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "26 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "27 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "28 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "29 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "30 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "31 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "32 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "33 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "34 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "35 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "36 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "37 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "38 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "39 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "40 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "41 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "42 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "43 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "44 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "45 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "46 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "47 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "48 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "49 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "50 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "51 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "52 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "53 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "54 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "55 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "56 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "57 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "58 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "59 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "60 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "61 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "62 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "63 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "64 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "65 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "66 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "67 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "68 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "69 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "70 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "71 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "72 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "73 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "74 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "75 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "76 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "77 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "78 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "79 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "80 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "81 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "82 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "83 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "84 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "85 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "86 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "87 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "88 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "89 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "90 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "91 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "92 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "93 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "94 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "95 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "96 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "97 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "98 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "99 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "100 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "101 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "102 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "103 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "104 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "105 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "106 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "107 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "108 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "109 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "110 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "111 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "112 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "113 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "114 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "115 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "116 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "117 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "118 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "119 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "120 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "121 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "122 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "123 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "124 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "125 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "126 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "127 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "128 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "129 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "130 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "131 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "132 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "133 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "134 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "135 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "136 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "137 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "138 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "139 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "140 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "141 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "142 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "143 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "144 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "145 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "146 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "147 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "148 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "149 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "150 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "151 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "152 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "153 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "154 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "155 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "156 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "157 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "158 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "159 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "160 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "161 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "162 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "163 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "164 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "165 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "166 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "167 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "168 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "169 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "170 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "171 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "172 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "173 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "174 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "175 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "176 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "177 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "178 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "179 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "180 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "181 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "182 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "183 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "184 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "185 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "186 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "187 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "188 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "189 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "190 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "191 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "192 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "193 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "194 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "195 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "196 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "197 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "198 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "199 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "200 3.484027 [[-0.24705672 -1.152827    0.40111333]\n",
      " [-0.14268851 -0.82493734 -0.7828744 ]\n",
      " [ 1.337713    0.25510642 -0.80939215]]\n",
      "Prediction: [2 0 0]\n",
      "Accuracy:  0.33333334\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a5604",
   "metadata": {},
   "source": [
    "# Non-normalized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1439283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b2592eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c56860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  11951182000.0 \n",
      "Prediction:\n",
      " [[ 77838.516]\n",
      " [156191.55 ]\n",
      " [122972.71 ]\n",
      " [ 86348.305]\n",
      " [101678.74 ]\n",
      " [102531.39 ]\n",
      " [ 94009.16 ]\n",
      " [119565.17 ]]\n",
      "1 Cost:  1.3130525e+25 \n",
      "Prediction:\n",
      " [[-2.5560678e+12]\n",
      " [-5.1456267e+12]\n",
      " [-4.0478785e+12]\n",
      " [-2.8375413e+12]\n",
      " [-3.3441941e+12]\n",
      " [-3.3723415e+12]\n",
      " [-3.0908678e+12]\n",
      " [-3.9352890e+12]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[8.4724320e+19]\n",
      " [1.7055874e+20]\n",
      " [1.3417241e+20]\n",
      " [9.4054151e+19]\n",
      " [1.1084784e+20]\n",
      " [1.1178082e+20]\n",
      " [1.0245100e+20]\n",
      " [1.3044048e+20]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[-2.8083022e+27]\n",
      " [-5.6534005e+27]\n",
      " [-4.4473261e+27]\n",
      " [-3.1175520e+27]\n",
      " [-3.6742015e+27]\n",
      " [-3.7051265e+27]\n",
      " [-3.3958768e+27]\n",
      " [-4.3236261e+27]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[9.3084970e+34]\n",
      " [1.8738958e+35]\n",
      " [1.4741262e+35]\n",
      " [1.0333547e+35]\n",
      " [1.2178637e+35]\n",
      " [1.2281143e+35]\n",
      " [1.1256091e+35]\n",
      " [1.4331242e+35]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5a6fd",
   "metadata": {},
   "source": [
    "# Normalized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63c362e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.53590045  0.56875715 -0.38043478  0.49104821  0.49485275]\n",
      " [ 0.24138537  0.27315268  0.61956522  0.20986603  0.33241067]\n",
      " [ 0.08002594  0.07150539  0.19565217  0.09751621  0.15548585]\n",
      " [-0.12519602 -0.11756261 -0.27173913 -0.04906045 -0.06713806]\n",
      " [ 0.05026046 -0.00541895 -0.07608696  0.07609626 -0.07890323]\n",
      " [ 0.03146225 -0.00541895 -0.06521739 -0.02764045 -0.01238587]\n",
      " [-0.3497389  -0.43124285 -0.17391304 -0.28887403 -0.31917486]\n",
      " [-0.46409954 -0.35377185  0.15217391 -0.50895179 -0.50514724]]\n"
     ]
    }
   ],
   "source": [
    "xy = MinMaxScaler(xy)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adee1142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  675365.75 \n",
      "Prediction:\n",
      " [[-1.944734  ]\n",
      " [-0.05629671]\n",
      " [-0.72870636]\n",
      " [-1.4599569 ]\n",
      " [-1.1954621 ]\n",
      " [-1.2043751 ]\n",
      " [-1.2077823 ]\n",
      " [-0.6355804 ]]\n",
      "1 Cost:  675338.75 \n",
      "Prediction:\n",
      " [[-1.9282357 ]\n",
      " [-0.03983277]\n",
      " [-0.7122612 ]\n",
      " [-1.443533  ]\n",
      " [-1.179022  ]\n",
      " [-1.1879396 ]\n",
      " [-1.1913888 ]\n",
      " [-0.6191969 ]]\n",
      "2 Cost:  675311.75 \n",
      "Prediction:\n",
      " [[-1.9117374 ]\n",
      " [-0.02336913]\n",
      " [-0.69581616]\n",
      " [-1.4271091 ]\n",
      " [-1.1625819 ]\n",
      " [-1.1715044 ]\n",
      " [-1.1749957 ]\n",
      " [-0.60281366]]\n",
      "3 Cost:  675284.75 \n",
      "Prediction:\n",
      " [[-1.8952398 ]\n",
      " [-0.00690603]\n",
      " [-0.6793716 ]\n",
      " [-1.4106857 ]\n",
      " [-1.1461424 ]\n",
      " [-1.1550695 ]\n",
      " [-1.1586027 ]\n",
      " [-0.5864308 ]]\n",
      "4 Cost:  675257.75 \n",
      "Prediction:\n",
      " [[-1.8787425 ]\n",
      " [ 0.00955695]\n",
      " [-0.6629273 ]\n",
      " [-1.3942626 ]\n",
      " [-1.129703  ]\n",
      " [-1.1386349 ]\n",
      " [-1.1422102 ]\n",
      " [-0.57004833]]\n",
      "5 Cost:  675230.7 \n",
      "Prediction:\n",
      " [[-1.8622456 ]\n",
      " [ 0.02601951]\n",
      " [-0.64648336]\n",
      " [-1.3778398 ]\n",
      " [-1.1132641 ]\n",
      " [-1.1222007 ]\n",
      " [-1.125818  ]\n",
      " [-0.55366606]]\n",
      "6 Cost:  675203.75 \n",
      "Prediction:\n",
      " [[-1.8457487 ]\n",
      " [ 0.04248184]\n",
      " [-0.63003975]\n",
      " [-1.3614174 ]\n",
      " [-1.0968255 ]\n",
      " [-1.1057668 ]\n",
      " [-1.1094263 ]\n",
      " [-0.5372842 ]]\n",
      "7 Cost:  675176.7 \n",
      "Prediction:\n",
      " [[-1.8292522 ]\n",
      " [ 0.05894369]\n",
      " [-0.61359644]\n",
      " [-1.3449953 ]\n",
      " [-1.0803871 ]\n",
      " [-1.0893332 ]\n",
      " [-1.0930347 ]\n",
      " [-0.52090263]]\n",
      "8 Cost:  675149.6 \n",
      "Prediction:\n",
      " [[-1.8127563 ]\n",
      " [ 0.07540524]\n",
      " [-0.5971535 ]\n",
      " [-1.3285735 ]\n",
      " [-1.0639492 ]\n",
      " [-1.0728999 ]\n",
      " [-1.0766436 ]\n",
      " [-0.50452137]]\n",
      "9 Cost:  675122.7 \n",
      "Prediction:\n",
      " [[-1.7962605 ]\n",
      " [ 0.09186655]\n",
      " [-0.5807108 ]\n",
      " [-1.312152  ]\n",
      " [-1.0475116 ]\n",
      " [-1.056467  ]\n",
      " [-1.0602527 ]\n",
      " [-0.48814052]]\n",
      "10 Cost:  675095.6 \n",
      "Prediction:\n",
      " [[-1.7797651 ]\n",
      " [ 0.10832751]\n",
      " [-0.5642686 ]\n",
      " [-1.295731  ]\n",
      " [-1.0310743 ]\n",
      " [-1.0400344 ]\n",
      " [-1.0438621 ]\n",
      " [-0.47175992]]\n",
      "11 Cost:  675068.7 \n",
      "Prediction:\n",
      " [[-1.76327   ]\n",
      " [ 0.12478811]\n",
      " [-0.5478265 ]\n",
      " [-1.2793101 ]\n",
      " [-1.0146372 ]\n",
      " [-1.0236021 ]\n",
      " [-1.0274719 ]\n",
      " [-0.45537966]]\n",
      "12 Cost:  675041.7 \n",
      "Prediction:\n",
      " [[-1.7467754 ]\n",
      " [ 0.14124835]\n",
      " [-0.53138494]\n",
      " [-1.2628896 ]\n",
      " [-0.99820065]\n",
      " [-1.0071702 ]\n",
      " [-1.011082  ]\n",
      " [-0.43899974]]\n",
      "13 Cost:  675014.6 \n",
      "Prediction:\n",
      " [[-1.7302809 ]\n",
      " [ 0.15770835]\n",
      " [-0.5149436 ]\n",
      " [-1.2464695 ]\n",
      " [-0.9817643 ]\n",
      " [-0.9907386 ]\n",
      " [-0.9946925 ]\n",
      " [-0.42262012]]\n",
      "14 Cost:  674987.75 \n",
      "Prediction:\n",
      " [[-1.713787  ]\n",
      " [ 0.17416799]\n",
      " [-0.4985026 ]\n",
      " [-1.2300497 ]\n",
      " [-0.96532834]\n",
      " [-0.9743073 ]\n",
      " [-0.9783033 ]\n",
      " [-0.40624088]]\n",
      "15 Cost:  674960.7 \n",
      "Prediction:\n",
      " [[-1.697293  ]\n",
      " [ 0.19062734]\n",
      " [-0.48206195]\n",
      " [-1.2136302 ]\n",
      " [-0.9488926 ]\n",
      " [-0.9578763 ]\n",
      " [-0.9619144 ]\n",
      " [-0.38986188]]\n",
      "16 Cost:  674933.7 \n",
      "Prediction:\n",
      " [[-1.6807997 ]\n",
      " [ 0.2070862 ]\n",
      " [-0.4656216 ]\n",
      " [-1.197211  ]\n",
      " [-0.9324573 ]\n",
      " [-0.9414457 ]\n",
      " [-0.9455258 ]\n",
      " [-0.37348333]]\n",
      "17 Cost:  674906.6 \n",
      "Prediction:\n",
      " [[-1.6643066 ]\n",
      " [ 0.2235449 ]\n",
      " [-0.44918162]\n",
      " [-1.1807921 ]\n",
      " [-0.9160223 ]\n",
      " [-0.9250154 ]\n",
      " [-0.9291375 ]\n",
      " [-0.35710502]]\n",
      "18 Cost:  674879.7 \n",
      "Prediction:\n",
      " [[-1.6478139 ]\n",
      " [ 0.24000317]\n",
      " [-0.43274194]\n",
      " [-1.1643736 ]\n",
      " [-0.89958763]\n",
      " [-0.9085854 ]\n",
      " [-0.91274965]\n",
      " [-0.3407271 ]]\n",
      "19 Cost:  674852.75 \n",
      "Prediction:\n",
      " [[-1.6313214 ]\n",
      " [ 0.2564612 ]\n",
      " [-0.41630256]\n",
      " [-1.1479555 ]\n",
      " [-0.8831532 ]\n",
      " [-0.89215577]\n",
      " [-0.89636207]\n",
      " [-0.3243494 ]]\n",
      "20 Cost:  674825.7 \n",
      "Prediction:\n",
      " [[-1.6148293 ]\n",
      " [ 0.27291882]\n",
      " [-0.39986357]\n",
      " [-1.1315377 ]\n",
      " [-0.86671925]\n",
      " [-0.87572646]\n",
      " [-0.8799748 ]\n",
      " [-0.30797207]]\n",
      "21 Cost:  674798.7 \n",
      "Prediction:\n",
      " [[-1.5983377 ]\n",
      " [ 0.28937614]\n",
      " [-0.38342482]\n",
      " [-1.1151202 ]\n",
      " [-0.85028553]\n",
      " [-0.85929745]\n",
      " [-0.86358786]\n",
      " [-0.2915951 ]]\n",
      "22 Cost:  674771.75 \n",
      "Prediction:\n",
      " [[-1.581846 ]\n",
      " [ 0.3058331]\n",
      " [-0.3669865]\n",
      " [-1.0987029]\n",
      " [-0.8338521]\n",
      " [-0.8428688]\n",
      " [-0.8472012]\n",
      " [-0.2752185]]\n",
      "23 Cost:  674744.75 \n",
      "Prediction:\n",
      " [[-1.5653551 ]\n",
      " [ 0.32228988]\n",
      " [-0.35054845]\n",
      " [-1.0822861 ]\n",
      " [-0.81741905]\n",
      " [-0.82644045]\n",
      " [-0.83081496]\n",
      " [-0.25884214]]\n",
      "24 Cost:  674717.75 \n",
      "Prediction:\n",
      " [[-1.5488641 ]\n",
      " [ 0.33874613]\n",
      " [-0.33411074]\n",
      " [-1.0658696 ]\n",
      " [-0.80098635]\n",
      " [-0.81001246]\n",
      " [-0.814429  ]\n",
      " [-0.24246615]]\n",
      "25 Cost:  674690.75 \n",
      "Prediction:\n",
      " [[-1.5323737 ]\n",
      " [ 0.35520214]\n",
      " [-0.31767333]\n",
      " [-1.0494533 ]\n",
      " [-0.784554  ]\n",
      " [-0.79358476]\n",
      " [-0.7980434 ]\n",
      " [-0.22609049]]\n",
      "26 Cost:  674663.75 \n",
      "Prediction:\n",
      " [[-1.5158836 ]\n",
      " [ 0.37165785]\n",
      " [-0.30123627]\n",
      " [-1.0330374 ]\n",
      " [-0.7681219 ]\n",
      " [-0.7771574 ]\n",
      " [-0.78165805]\n",
      " [-0.20971513]]\n",
      "27 Cost:  674636.8 \n",
      "Prediction:\n",
      " [[-1.4993938 ]\n",
      " [ 0.38811326]\n",
      " [-0.28479955]\n",
      " [-1.0166218 ]\n",
      " [-0.75169015]\n",
      " [-0.7607304 ]\n",
      " [-0.7652731 ]\n",
      " [-0.19334015]]\n",
      "28 Cost:  674609.75 \n",
      "Prediction:\n",
      " [[-1.4829043 ]\n",
      " [ 0.4045682 ]\n",
      " [-0.26836318]\n",
      " [-1.0002067 ]\n",
      " [-0.73525876]\n",
      " [-0.7443037 ]\n",
      " [-0.7488885 ]\n",
      " [-0.17696542]]\n",
      "29 Cost:  674582.8 \n",
      "Prediction:\n",
      " [[-1.4664152 ]\n",
      " [ 0.4210229 ]\n",
      " [-0.25192714]\n",
      " [-0.9837918 ]\n",
      " [-0.7188277 ]\n",
      " [-0.7278774 ]\n",
      " [-0.7325042 ]\n",
      " [-0.16059113]]\n",
      "30 Cost:  674555.9 \n",
      "Prediction:\n",
      " [[-1.4499263 ]\n",
      " [ 0.4374773 ]\n",
      " [-0.2354914 ]\n",
      " [-0.96737725]\n",
      " [-0.702397  ]\n",
      " [-0.71145135]\n",
      " [-0.71612024]\n",
      " [-0.14421713]]\n",
      "31 Cost:  674528.9 \n",
      "Prediction:\n",
      " [[-1.4334378 ]\n",
      " [ 0.45393127]\n",
      " [-0.21905604]\n",
      " [-0.950963  ]\n",
      " [-0.6859666 ]\n",
      " [-0.6950257 ]\n",
      " [-0.6997366 ]\n",
      " [-0.12784353]]\n",
      "32 Cost:  674501.9 \n",
      "Prediction:\n",
      " [[-1.4169497 ]\n",
      " [ 0.4703849 ]\n",
      " [-0.20262092]\n",
      " [-0.9345491 ]\n",
      " [-0.66953653]\n",
      " [-0.6786003 ]\n",
      " [-0.6833533 ]\n",
      " [-0.1114701 ]]\n",
      "33 Cost:  674474.94 \n",
      "Prediction:\n",
      " [[-1.4004619 ]\n",
      " [ 0.48683834]\n",
      " [-0.18618625]\n",
      " [-0.91813564]\n",
      " [-0.6531068 ]\n",
      " [-0.6621753 ]\n",
      " [-0.6669704 ]\n",
      " [-0.09509712]]\n",
      "34 Cost:  674447.9 \n",
      "Prediction:\n",
      " [[-1.3839743 ]\n",
      " [ 0.50329137]\n",
      " [-0.1697518 ]\n",
      " [-0.9017223 ]\n",
      " [-0.6366774 ]\n",
      " [-0.6457506 ]\n",
      " [-0.6505877 ]\n",
      " [-0.07872441]]\n",
      "35 Cost:  674420.94 \n",
      "Prediction:\n",
      " [[-1.3674873 ]\n",
      " [ 0.51974416]\n",
      " [-0.15331772]\n",
      " [-0.88530946]\n",
      " [-0.6202483 ]\n",
      " [-0.6293262 ]\n",
      " [-0.63420534]\n",
      " [-0.06235197]]\n",
      "36 Cost:  674394.0 \n",
      "Prediction:\n",
      " [[-1.3510004 ]\n",
      " [ 0.5361965 ]\n",
      " [-0.13688397]\n",
      " [-0.86889684]\n",
      " [-0.6038195 ]\n",
      " [-0.61290216]\n",
      " [-0.61782336]\n",
      " [-0.04598001]]\n",
      "37 Cost:  674367.0 \n",
      "Prediction:\n",
      " [[-1.3345139 ]\n",
      " [ 0.55264854]\n",
      " [-0.1204505 ]\n",
      " [-0.8524846 ]\n",
      " [-0.5873911 ]\n",
      " [-0.5964784 ]\n",
      " [-0.6014416 ]\n",
      " [-0.02960825]]\n",
      "38 Cost:  674340.06 \n",
      "Prediction:\n",
      " [[-1.3180277 ]\n",
      " [ 0.56910026]\n",
      " [-0.10401741]\n",
      " [-0.8360727 ]\n",
      " [-0.57096297]\n",
      " [-0.580055  ]\n",
      " [-0.58506036]\n",
      " [-0.01323679]]\n",
      "39 Cost:  674313.1 \n",
      "Prediction:\n",
      " [[-1.3015419 ]\n",
      " [ 0.58555174]\n",
      " [-0.08758461]\n",
      " [-0.819661  ]\n",
      " [-0.5545352 ]\n",
      " [-0.5636319 ]\n",
      " [-0.5686793 ]\n",
      " [ 0.00313422]]\n",
      "40 Cost:  674286.1 \n",
      "Prediction:\n",
      " [[-1.2850562 ]\n",
      " [ 0.60200286]\n",
      " [-0.07115218]\n",
      " [-0.8032497 ]\n",
      " [-0.53810775]\n",
      " [-0.54720914]\n",
      " [-0.5522986 ]\n",
      " [ 0.01950499]]\n",
      "41 Cost:  674259.1 \n",
      "Prediction:\n",
      " [[-1.2685711 ]\n",
      " [ 0.6184535 ]\n",
      " [-0.05472004]\n",
      " [-0.78683877]\n",
      " [-0.5216806 ]\n",
      " [-0.53078675]\n",
      " [-0.53591824]\n",
      " [ 0.03587532]]\n",
      "42 Cost:  674232.1 \n",
      "Prediction:\n",
      " [[-1.2520862 ]\n",
      " [ 0.63490397]\n",
      " [-0.03828824]\n",
      " [-0.7704282 ]\n",
      " [-0.5052538 ]\n",
      " [-0.51436466]\n",
      " [-0.51953816]\n",
      " [ 0.05224547]]\n",
      "43 Cost:  674205.1 \n",
      "Prediction:\n",
      " [[-1.2356017 ]\n",
      " [ 0.6513541 ]\n",
      " [-0.02185678]\n",
      " [-0.7540179 ]\n",
      " [-0.48882732]\n",
      " [-0.4979429 ]\n",
      " [-0.5031585 ]\n",
      " [ 0.06861523]]\n",
      "44 Cost:  674178.2 \n",
      "Prediction:\n",
      " [[-1.2191174 ]\n",
      " [ 0.6678039 ]\n",
      " [-0.0054256 ]\n",
      " [-0.73760796]\n",
      " [-0.4724012 ]\n",
      " [-0.48152146]\n",
      " [-0.4867791 ]\n",
      " [ 0.08498463]]\n",
      "45 Cost:  674151.25 \n",
      "Prediction:\n",
      " [[-1.2026336 ]\n",
      " [ 0.6842533 ]\n",
      " [ 0.01100516]\n",
      " [-0.7211983 ]\n",
      " [-0.45597535]\n",
      " [-0.46510035]\n",
      " [-0.47040004]\n",
      " [ 0.10135373]]\n",
      "46 Cost:  674124.3 \n",
      "Prediction:\n",
      " [[-1.1861501 ]\n",
      " [ 0.7007024 ]\n",
      " [ 0.02743566]\n",
      " [-0.7047889 ]\n",
      " [-0.43954986]\n",
      " [-0.44867957]\n",
      " [-0.4540213 ]\n",
      " [ 0.11772251]]\n",
      "47 Cost:  674097.25 \n",
      "Prediction:\n",
      " [[-1.1696666 ]\n",
      " [ 0.71715117]\n",
      " [ 0.04386583]\n",
      " [-0.68838   ]\n",
      " [-0.4231247 ]\n",
      " [-0.4322591 ]\n",
      " [-0.43764293]\n",
      " [ 0.13409099]]\n",
      "48 Cost:  674070.4 \n",
      "Prediction:\n",
      " [[-1.1531838 ]\n",
      " [ 0.73359966]\n",
      " [ 0.06029567]\n",
      " [-0.6719713 ]\n",
      " [-0.40669987]\n",
      " [-0.415839  ]\n",
      " [-0.42126486]\n",
      " [ 0.15045902]]\n",
      "49 Cost:  674043.4 \n",
      "Prediction:\n",
      " [[-1.1367012 ]\n",
      " [ 0.75004774]\n",
      " [ 0.07672514]\n",
      " [-0.655563  ]\n",
      " [-0.39027536]\n",
      " [-0.3994192 ]\n",
      " [-0.4048871 ]\n",
      " [ 0.16682686]]\n",
      "50 Cost:  674016.44 \n",
      "Prediction:\n",
      " [[-1.1202191 ]\n",
      " [ 0.7664956 ]\n",
      " [ 0.09315428]\n",
      " [-0.63915503]\n",
      " [-0.37385118]\n",
      " [-0.38299975]\n",
      " [-0.38850972]\n",
      " [ 0.18319425]]\n",
      "51 Cost:  673989.5 \n",
      "Prediction:\n",
      " [[-1.1037371 ]\n",
      " [ 0.782943  ]\n",
      " [ 0.10958312]\n",
      " [-0.6227473 ]\n",
      " [-0.35742736]\n",
      " [-0.3665806 ]\n",
      " [-0.37213266]\n",
      " [ 0.1995614 ]]\n",
      "52 Cost:  673962.5 \n",
      "Prediction:\n",
      " [[-1.0872556 ]\n",
      " [ 0.7993902 ]\n",
      " [ 0.12601164]\n",
      " [-0.60634005]\n",
      " [-0.34100387]\n",
      " [-0.35016182]\n",
      " [-0.35575593]\n",
      " [ 0.21592817]]\n",
      "53 Cost:  673935.5 \n",
      "Prediction:\n",
      " [[-1.0707743 ]\n",
      " [ 0.8158369 ]\n",
      " [ 0.14243983]\n",
      " [-0.58993304]\n",
      " [-0.32458067]\n",
      " [-0.33374333]\n",
      " [-0.3393795 ]\n",
      " [ 0.23229463]]\n",
      "54 Cost:  673908.6 \n",
      "Prediction:\n",
      " [[-1.0542934 ]\n",
      " [ 0.8322834 ]\n",
      " [ 0.15886766]\n",
      " [-0.5735264 ]\n",
      " [-0.3081578 ]\n",
      " [-0.3173252 ]\n",
      " [-0.32300338]\n",
      " [ 0.24866077]]\n",
      "55 Cost:  673881.6 \n",
      "Prediction:\n",
      " [[-1.0378127 ]\n",
      " [ 0.8487296 ]\n",
      " [ 0.17529522]\n",
      " [-0.55712   ]\n",
      " [-0.2917353 ]\n",
      " [-0.30090737]\n",
      " [-0.30662763]\n",
      " [ 0.26502657]]\n",
      "56 Cost:  673854.75 \n",
      "Prediction:\n",
      " [[-1.0213325 ]\n",
      " [ 0.86517537]\n",
      " [ 0.19172236]\n",
      " [-0.540714  ]\n",
      " [-0.27531308]\n",
      " [-0.2844899 ]\n",
      " [-0.2902522 ]\n",
      " [ 0.2813921 ]]\n",
      "57 Cost:  673827.8 \n",
      "Prediction:\n",
      " [[-1.0048525 ]\n",
      " [ 0.8816208 ]\n",
      " [ 0.20814924]\n",
      " [-0.52430826]\n",
      " [-0.25889122]\n",
      " [-0.26807272]\n",
      " [-0.27387705]\n",
      " [ 0.29775727]]\n",
      "58 Cost:  673800.75 \n",
      "Prediction:\n",
      " [[-0.98837304]\n",
      " [ 0.89806604]\n",
      " [ 0.22457579]\n",
      " [-0.5079029 ]\n",
      " [-0.2424697 ]\n",
      " [-0.2516559 ]\n",
      " [-0.2575023 ]\n",
      " [ 0.31412205]]\n",
      "59 Cost:  673773.9 \n",
      "Prediction:\n",
      " [[-0.9718936 ]\n",
      " [ 0.9145109 ]\n",
      " [ 0.24100198]\n",
      " [-0.49149787]\n",
      " [-0.22604847]\n",
      " [-0.2352394 ]\n",
      " [-0.24112783]\n",
      " [ 0.33048654]]\n",
      "60 Cost:  673746.9 \n",
      "Prediction:\n",
      " [[-0.9554148 ]\n",
      " [ 0.9309554 ]\n",
      " [ 0.25742787]\n",
      " [-0.4750932 ]\n",
      " [-0.20962757]\n",
      " [-0.21882322]\n",
      " [-0.22475371]\n",
      " [ 0.3468507 ]]\n",
      "61 Cost:  673719.94 \n",
      "Prediction:\n",
      " [[-0.93893605]\n",
      " [ 0.9473996 ]\n",
      " [ 0.2738534 ]\n",
      " [-0.45868883]\n",
      " [-0.19320703]\n",
      " [-0.20240736]\n",
      " [-0.20837992]\n",
      " [ 0.36321458]]\n",
      "62 Cost:  673693.0 \n",
      "Prediction:\n",
      " [[-0.9224578 ]\n",
      " [ 0.9638434 ]\n",
      " [ 0.29027864]\n",
      " [-0.44228476]\n",
      " [-0.17678678]\n",
      " [-0.18599185]\n",
      " [-0.19200644]\n",
      " [ 0.37957802]]\n",
      "63 Cost:  673666.1 \n",
      "Prediction:\n",
      " [[-0.90597975]\n",
      " [ 0.98028696]\n",
      " [ 0.30670354]\n",
      " [-0.42588103]\n",
      " [-0.1603669 ]\n",
      " [-0.16957666]\n",
      " [-0.17563334]\n",
      " [ 0.39594123]]\n",
      "64 Cost:  673639.1 \n",
      "Prediction:\n",
      " [[-0.8895022 ]\n",
      " [ 0.99673015]\n",
      " [ 0.3231281 ]\n",
      " [-0.40947765]\n",
      " [-0.14394732]\n",
      " [-0.15316181]\n",
      " [-0.1592605 ]\n",
      " [ 0.412304  ]]\n",
      "65 Cost:  673612.1 \n",
      "Prediction:\n",
      " [[-0.8730249 ]\n",
      " [ 1.0131731 ]\n",
      " [ 0.33955234]\n",
      " [-0.39307457]\n",
      " [-0.12752807]\n",
      " [-0.13674727]\n",
      " [-0.14288804]\n",
      " [ 0.42866662]]\n",
      "66 Cost:  673585.25 \n",
      "Prediction:\n",
      " [[-0.85654783]\n",
      " [ 1.0296155 ]\n",
      " [ 0.35597625]\n",
      " [-0.37667182]\n",
      " [-0.11110917]\n",
      " [-0.12033306]\n",
      " [-0.1265159 ]\n",
      " [ 0.44502884]]\n",
      "67 Cost:  673558.25 \n",
      "Prediction:\n",
      " [[-0.8400712 ]\n",
      " [ 1.0460577 ]\n",
      " [ 0.3723998 ]\n",
      " [-0.36026946]\n",
      " [-0.09469058]\n",
      " [-0.10391918]\n",
      " [-0.11014406]\n",
      " [ 0.46139067]]\n",
      "68 Cost:  673531.3 \n",
      "Prediction:\n",
      " [[-0.8235948 ]\n",
      " [ 1.0624996 ]\n",
      " [ 0.38882306]\n",
      " [-0.34386736]\n",
      " [-0.07827232]\n",
      " [-0.08750565]\n",
      " [-0.09377257]\n",
      " [ 0.47775227]]\n",
      "69 Cost:  673504.4 \n",
      "Prediction:\n",
      " [[-0.80711883]\n",
      " [ 1.0789412 ]\n",
      " [ 0.40524602]\n",
      " [-0.3274656 ]\n",
      " [-0.06185439]\n",
      " [-0.07109243]\n",
      " [-0.07740138]\n",
      " [ 0.49411345]]\n",
      "70 Cost:  673477.44 \n",
      "Prediction:\n",
      " [[-0.7906431 ]\n",
      " [ 1.0953823 ]\n",
      " [ 0.4216686 ]\n",
      " [-0.31106418]\n",
      " [-0.04543679]\n",
      " [-0.05467952]\n",
      " [-0.06103057]\n",
      " [ 0.5104743 ]]\n",
      "71 Cost:  673450.5 \n",
      "Prediction:\n",
      " [[-0.77416784]\n",
      " [ 1.1118233 ]\n",
      " [ 0.43809092]\n",
      " [-0.29466307]\n",
      " [-0.02901952]\n",
      " [-0.03826696]\n",
      " [-0.04466005]\n",
      " [ 0.5268349 ]]\n",
      "72 Cost:  673423.56 \n",
      "Prediction:\n",
      " [[-0.7576928 ]\n",
      " [ 1.128264  ]\n",
      " [ 0.45451277]\n",
      " [-0.27826232]\n",
      " [-0.01260258]\n",
      " [-0.02185473]\n",
      " [-0.02828987]\n",
      " [ 0.5431951 ]]\n",
      "73 Cost:  673396.6 \n",
      "Prediction:\n",
      " [[-0.7412182 ]\n",
      " [ 1.1447041 ]\n",
      " [ 0.4709344 ]\n",
      " [-0.26186192]\n",
      " [ 0.00381403]\n",
      " [-0.00544283]\n",
      " [-0.01192005]\n",
      " [ 0.559555  ]]\n",
      "74 Cost:  673369.75 \n",
      "Prediction:\n",
      " [[-0.7247437 ]\n",
      " [ 1.161144  ]\n",
      " [ 0.4873557 ]\n",
      " [-0.24546178]\n",
      " [ 0.02023031]\n",
      " [ 0.01096874]\n",
      " [ 0.00444952]\n",
      " [ 0.57591456]]\n",
      "75 Cost:  673342.75 \n",
      "Prediction:\n",
      " [[-0.7082698 ]\n",
      " [ 1.1775836 ]\n",
      " [ 0.5037766 ]\n",
      " [-0.229062  ]\n",
      " [ 0.03664626]\n",
      " [ 0.02737999]\n",
      " [ 0.0208187 ]\n",
      " [ 0.5922738 ]]\n",
      "76 Cost:  673315.9 \n",
      "Prediction:\n",
      " [[-0.69179606]\n",
      " [ 1.1940228 ]\n",
      " [ 0.5201973 ]\n",
      " [-0.21266253]\n",
      " [ 0.05306189]\n",
      " [ 0.04379091]\n",
      " [ 0.03718753]\n",
      " [ 0.60863274]]\n",
      "77 Cost:  673289.0 \n",
      "Prediction:\n",
      " [[-0.6753227 ]\n",
      " [ 1.2104619 ]\n",
      " [ 0.5366176 ]\n",
      " [-0.19626345]\n",
      " [ 0.06947722]\n",
      " [ 0.06020148]\n",
      " [ 0.05355607]\n",
      " [ 0.62499124]]\n",
      "78 Cost:  673262.0 \n",
      "Prediction:\n",
      " [[-0.6588497 ]\n",
      " [ 1.2269003 ]\n",
      " [ 0.5530375 ]\n",
      " [-0.17986466]\n",
      " [ 0.08589216]\n",
      " [ 0.07661176]\n",
      " [ 0.06992432]\n",
      " [ 0.64134955]]\n",
      "79 Cost:  673235.1 \n",
      "Prediction:\n",
      " [[-0.64237696]\n",
      " [ 1.2433386 ]\n",
      " [ 0.5694572 ]\n",
      " [-0.1634662 ]\n",
      " [ 0.10230681]\n",
      " [ 0.09302169]\n",
      " [ 0.08629222]\n",
      " [ 0.65770745]]\n",
      "80 Cost:  673208.1 \n",
      "Prediction:\n",
      " [[-0.62590456]\n",
      " [ 1.2597765 ]\n",
      " [ 0.58587646]\n",
      " [-0.147068  ]\n",
      " [ 0.11872113]\n",
      " [ 0.10943131]\n",
      " [ 0.10265976]\n",
      " [ 0.6740651 ]]\n",
      "81 Cost:  673181.25 \n",
      "Prediction:\n",
      " [[-0.60943246]\n",
      " [ 1.2762141 ]\n",
      " [ 0.60229546]\n",
      " [-0.13067019]\n",
      " [ 0.13513513]\n",
      " [ 0.12584059]\n",
      " [ 0.11902699]\n",
      " [ 0.6904224 ]]\n",
      "82 Cost:  673154.25 \n",
      "Prediction:\n",
      " [[-0.59296083]\n",
      " [ 1.2926514 ]\n",
      " [ 0.6187141 ]\n",
      " [-0.11427271]\n",
      " [ 0.1515488 ]\n",
      " [ 0.14224954]\n",
      " [ 0.1353939 ]\n",
      " [ 0.70677924]]\n",
      "83 Cost:  673127.4 \n",
      "Prediction:\n",
      " [[-0.5764894 ]\n",
      " [ 1.3090883 ]\n",
      " [ 0.63513243]\n",
      " [-0.0978756 ]\n",
      " [ 0.1679621 ]\n",
      " [ 0.15865818]\n",
      " [ 0.15176046]\n",
      " [ 0.72313595]]\n",
      "84 Cost:  673100.4 \n",
      "Prediction:\n",
      " [[-0.5600182 ]\n",
      " [ 1.325525  ]\n",
      " [ 0.6515504 ]\n",
      " [-0.08147877]\n",
      " [ 0.18437512]\n",
      " [ 0.17506646]\n",
      " [ 0.1681267 ]\n",
      " [ 0.7394922 ]]\n",
      "85 Cost:  673073.5 \n",
      "Prediction:\n",
      " [[-0.5435475 ]\n",
      " [ 1.3419614 ]\n",
      " [ 0.66796815]\n",
      " [-0.06508228]\n",
      " [ 0.2007878 ]\n",
      " [ 0.19147442]\n",
      " [ 0.18449262]\n",
      " [ 0.75584817]]\n",
      "86 Cost:  673046.6 \n",
      "Prediction:\n",
      " [[-0.5270772 ]\n",
      " [ 1.3583974 ]\n",
      " [ 0.6843854 ]\n",
      " [-0.04868606]\n",
      " [ 0.21720016]\n",
      " [ 0.20788208]\n",
      " [ 0.20085819]\n",
      " [ 0.7722038 ]]\n",
      "87 Cost:  673019.7 \n",
      "Prediction:\n",
      " [[-0.51060706]\n",
      " [ 1.3748329 ]\n",
      " [ 0.70080245]\n",
      " [-0.03229022]\n",
      " [ 0.23361218]\n",
      " [ 0.22428939]\n",
      " [ 0.2172235 ]\n",
      " [ 0.7885591 ]]\n",
      "88 Cost:  672992.75 \n",
      "Prediction:\n",
      " [[-0.49413726]\n",
      " [ 1.3912683 ]\n",
      " [ 0.7172192 ]\n",
      " [-0.01589474]\n",
      " [ 0.25002387]\n",
      " [ 0.24069636]\n",
      " [ 0.2335884 ]\n",
      " [ 0.8049141 ]]\n",
      "89 Cost:  672965.9 \n",
      "Prediction:\n",
      " [[-4.7766784e-01]\n",
      " [ 1.4077033e+00]\n",
      " [ 7.3363554e-01]\n",
      " [ 5.0044060e-04]\n",
      " [ 2.6643521e-01]\n",
      " [ 2.5710303e-01]\n",
      " [ 2.4995297e-01]\n",
      " [ 8.2126868e-01]]\n",
      "90 Cost:  672938.9 \n",
      "Prediction:\n",
      " [[-0.4611988 ]\n",
      " [ 1.4241381 ]\n",
      " [ 0.7500515 ]\n",
      " [ 0.01689529]\n",
      " [ 0.28284627]\n",
      " [ 0.27350935]\n",
      " [ 0.2663173 ]\n",
      " [ 0.837623  ]]\n",
      "91 Cost:  672912.0 \n",
      "Prediction:\n",
      " [[-0.44472998]\n",
      " [ 1.4405721 ]\n",
      " [ 0.7664672 ]\n",
      " [ 0.03328985]\n",
      " [ 0.29925698]\n",
      " [ 0.28991535]\n",
      " [ 0.28268123]\n",
      " [ 0.853977  ]]\n",
      "92 Cost:  672885.06 \n",
      "Prediction:\n",
      " [[-0.42826143]\n",
      " [ 1.4570062 ]\n",
      " [ 0.78288263]\n",
      " [ 0.04968408]\n",
      " [ 0.31566736]\n",
      " [ 0.30632102]\n",
      " [ 0.29904488]\n",
      " [ 0.8703307 ]]\n",
      "93 Cost:  672858.1 \n",
      "Prediction:\n",
      " [[-0.41179338]\n",
      " [ 1.4734399 ]\n",
      " [ 0.7992977 ]\n",
      " [ 0.06607795]\n",
      " [ 0.3320774 ]\n",
      " [ 0.32272637]\n",
      " [ 0.31540814]\n",
      " [ 0.88668406]]\n",
      "94 Cost:  672831.25 \n",
      "Prediction:\n",
      " [[-0.39532566]\n",
      " [ 1.4898733 ]\n",
      " [ 0.8157124 ]\n",
      " [ 0.08247149]\n",
      " [ 0.34848714]\n",
      " [ 0.33913139]\n",
      " [ 0.33177108]\n",
      " [ 0.9030371 ]]\n",
      "95 Cost:  672804.4 \n",
      "Prediction:\n",
      " [[-0.37885815]\n",
      " [ 1.5063062 ]\n",
      " [ 0.83212674]\n",
      " [ 0.09886467]\n",
      " [ 0.36489654]\n",
      " [ 0.35553607]\n",
      " [ 0.34813377]\n",
      " [ 0.9193897 ]]\n",
      "96 Cost:  672777.4 \n",
      "Prediction:\n",
      " [[-0.36239105]\n",
      " [ 1.5227389 ]\n",
      " [ 0.8485408 ]\n",
      " [ 0.11525756]\n",
      " [ 0.38130558]\n",
      " [ 0.3719404 ]\n",
      " [ 0.36449602]\n",
      " [ 0.9357421 ]]\n",
      "97 Cost:  672750.5 \n",
      "Prediction:\n",
      " [[-0.3459242 ]\n",
      " [ 1.5391713 ]\n",
      " [ 0.8649546 ]\n",
      " [ 0.13165015]\n",
      " [ 0.39771432]\n",
      " [ 0.38834444]\n",
      " [ 0.380858  ]\n",
      " [ 0.9520941 ]]\n",
      "98 Cost:  672723.6 \n",
      "Prediction:\n",
      " [[-0.32945782]\n",
      " [ 1.5556034 ]\n",
      " [ 0.8813679 ]\n",
      " [ 0.14804235]\n",
      " [ 0.4141227 ]\n",
      " [ 0.4047481 ]\n",
      " [ 0.39721966]\n",
      " [ 0.9684458 ]]\n",
      "99 Cost:  672696.75 \n",
      "Prediction:\n",
      " [[-0.31299156]\n",
      " [ 1.5720351 ]\n",
      " [ 0.897781  ]\n",
      " [ 0.16443425]\n",
      " [ 0.4305308 ]\n",
      " [ 0.4211515 ]\n",
      " [ 0.41358095]\n",
      " [ 0.9847971 ]]\n",
      "100 Cost:  672669.8 \n",
      "Prediction:\n",
      " [[-0.29652584]\n",
      " [ 1.5884663 ]\n",
      " [ 0.91419375]\n",
      " [ 0.18082583]\n",
      " [ 0.4469385 ]\n",
      " [ 0.4375545 ]\n",
      " [ 0.42994198]\n",
      " [ 1.0011481 ]]\n"
     ]
    }
   ],
   "source": [
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
